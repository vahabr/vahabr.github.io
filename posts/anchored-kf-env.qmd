---
title: "Anchored Kalman Filtering – Reproducible Environment"
format:
  html:
    code-tools: true     # show Copy/Download/Run buttons in HTML
    thebe: true          # enable live execution in browser via Thebe
execute:
  echo: true
  warning: false
  cache: false
jupyter: python3
---


## Overview

This page sets up a **deterministic Python environment** for experiments around **anchored Kalman filtering** in a **Markov PD model**, using only NumPy, Pandas, and Matplotlib.
Each code block is explained in markdown before the code itself.

---

## Imports and Basic Configuration

We import the required libraries and use `pathlib.Path` for file handling.

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Tuple, Dict

```

---

## Output Directory

For portability, we create a local folder `outputs/` that works in Quarto or Thebe.

```{python}
OUTDIR = Path("outputs")
OUTDIR.mkdir(parents=True, exist_ok=True)
OUTDIR.resolve()
```

---

## Ratings Universe and Defaults

We define the rating states, their index mapping, and some defaults:

* `T_DEFAULT`: time horizon (quarters)
* `N_DEFAULT`: number of obligors

```{python}
RATINGS = ["A", "B", "C", "D"]
IDX = {r: i for i, r in enumerate(RATINGS)}
T_DEFAULT = 20    # quarters
N_DEFAULT = 10_000
RATINGS, IDX, T_DEFAULT, N_DEFAULT
```

---

## 1) Determinism Helper

To make runs reproducible, we fix the random seed and return a local RNG (`numpy.random.Generator`).

```{python}
def set_seed(seed: int) -> np.random.Generator:
    """Set global determinism and return a local RNG."""
    np.random.seed(seed)
    return np.random.default_rng(seed)

# Example usage
rng = set_seed(42)
rng
```

---

## 2) Portfolio Initialisation

This function samples initial ratings for `N` obligors based on an input distribution `pi0`.

```{python}
def gen_initial_portfolio(N: int, pi0: np.ndarray, rng: np.random.Generator=None) -> np.ndarray:
    """Sample initial ratings for N obligors from distribution pi0."""
    if rng is None:
        rng = np.random.default_rng()
    labels = rng.choice(len(RATINGS), size=N, p=pi0)
    return labels
```

Demo: initialise a portfolio with a skew toward A and B.

```{python}
pi0 = np.array([0.50, 0.35, 0.10, 0.05], dtype=float)
labels0 = gen_initial_portfolio(N_DEFAULT, pi0, rng)

(pd.Series(labels0)
   .map({i:r for r,i in IDX.items()})
   .value_counts(normalize=True)
   .rename("share")
   .to_frame()
   .sort_index())
```

---

## 3) Through-the-Cycle (TTC) Transition Matrix

We construct the quarterly TTC matrix for states `A, B, C, D` with `D` absorbing.

```{python}
def build_P_TTC() -> np.ndarray:
    """Return the quarterly TTC transition matrix."""
    P_TTC = np.array([
        [0.975, 0.022, 0.002, 0.001],
        [0.030, 0.935, 0.030, 0.005],
        [0.010, 0.060, 0.915, 0.015],
        [0.000, 0.000, 0.000, 1.000],
    ], dtype=float)
    return P_TTC

P_TTC = build_P_TTC()
P_TTC
```

Verify that rows sum to 1 and visualise the transition matrix.

```{python}
row_sums = P_TTC.sum(axis=1)
assert np.allclose(row_sums, 1.0), "Each row must sum to 1."

fig, ax = plt.subplots(figsize=(4, 3))
im = ax.imshow(P_TTC, aspect="auto")
ax.set_xticks(range(len(RATINGS))); ax.set_xticklabels(RATINGS)
ax.set_yticks(range(len(RATINGS))); ax.set_yticklabels(RATINGS)
ax.set_title("Quarterly TTC Transition Matrix")
fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
plt.show()
```

---

### Minimal Sanity Run

A helper to advance the portfolio one quarter using the TTC matrix.
This confirms our setup is consistent.

```{python}
def step_markov(labels: np.ndarray, P: np.ndarray, rng: np.random.Generator) -> np.ndarray:
    """Advance one step in a Markov chain with transition matrix P."""
    n_states = P.shape[0]
    u = rng.random(size=labels.shape[0])
    next_labels = np.empty_like(labels)
    cdf = P.cumsum(axis=1)
    for s in range(n_states):
        mask = (labels == s)
        if not np.any(mask):
            continue
        next_labels[mask] = np.searchsorted(cdf[s], u[mask], side="right")
    return next_labels

labels1 = step_markov(labels0, P_TTC, rng)

pd.crosstab(
    pd.Series(labels0).map({i:r for r,i in IDX.items()}),
    pd.Series(labels1).map({i:r for r,i in IDX.items()}),
    normalize="index"
).round(3)
```

---


Perfect — that’s the **Macroeconomic Scenarios** block. To integrate it into your Quarto page the same way we did with Sections 1–3, we just add **markdown explanations before each function group** and keep the code in fenced `{python}` blocks.

Here’s the `.qmd`-ready section you can paste directly under the earlier parts:

---

## 4) Macroeconomic Scenarios

We create three stylised macroeconomic scenarios:

* **Baseline**: mild cycle with small fluctuations in GDP and unemployment.
* **Stress**: sharp downturn followed by a slow recovery.
* **Pandemic**: abrupt fall, fast rebound, with a short spike in unemployment.

Each scenario returns quarterly paths for GDP growth and unemployment.

```{python}
def _baseline_paths(T: int) -> Tuple[np.ndarray, np.ndarray]:
    """Baseline: mild cycle. GDP in pp q/q, Unemp level in pp."""
    t = np.arange(T)
    gdp = 0.5 + 0.3*np.sin(2*np.pi*t/12)  # around 0.5% q/q, small cycle
    unemp = 5.0 + 0.2*np.cos(2*np.pi*(t+3)/10)  # around 5%, small wiggle
    return gdp, unemp

def _stress_paths(T: int) -> Tuple[np.ndarray, np.ndarray]:
    """Stress: sharp downturn, slow recovery."""
    t = np.arange(T)
    gdp = 0.6 - 0.8*np.exp(-t/2.0)  # drop quickly then recover slowly
    gdp[:4] -= 0.8  # accentuate early downturn
    unemp = 4.5 + 1.8*(1 - np.exp(-t/4.0))  # rises then plateaus
    return gdp, unemp

def _pandemic_paths(T: int) -> Tuple[np.ndarray, np.ndarray]:
    """Pandemic: abrupt fall, fast rebound (impose rebound at t=3 later)."""
    t = np.arange(T)
    gdp = 0.5*np.ones(T)
    gdp[0:2] = -2.0  # abrupt fall
    gdp[2] = -0.5
    # Rebound will be imposed at t=3 in realised series
    unemp = 4.2*np.ones(T)
    unemp[0:3] = 5.5  # spike
    unemp[3:6] = 5.0
    return gdp, unemp
```

---

### Forecasts vs Realisations

We simulate both **forecasts** (scenario paths) and **realisations** (forecast + Gaussian noise).
For the pandemic scenario, we impose a strong rebound at quarter 3.
We also construct a **macro index** from GDP and unemployment.

```{python}
def gen_macro_forecasts_and_realised(
    scenario: str, T: int = T_DEFAULT, rng: np.random.Generator = None
) -> Dict[str, np.ndarray]:
    """Return dict with forecast and realised GDP/UNEMP and the macro index paths."""
    if rng is None:
        rng = np.random.default_rng()
    scenario = scenario.lower()
    if scenario == "baseline":
        gdp_f, unemp_f = _baseline_paths(T)
    elif scenario == "stress":
        gdp_f, unemp_f = _stress_paths(T)
    elif scenario in {"pandemic", "pandemic_shock"}:
        gdp_f, unemp_f = _pandemic_paths(T)
    else:
        raise ValueError("Unknown scenario")

    # Realised paths = forecast + Gaussian noise (σ ≈ 0.2pp)
    gdp_r = gdp_f + rng.normal(0.0, 0.2, size=T)
    unemp_r = unemp_f + rng.normal(0.0, 0.2, size=T)

    # Pandemic rebound at t=3
    if scenario in {"pandemic", "pandemic_shock"} and T >= 4:
        gdp_r[3] += 2.5
        unemp_r[3] -= 0.8

    # Macro indices
    M_hat = macro_index_from_gdp_unemp(gdp_f, unemp_f)
    M_real = macro_index_from_gdp_unemp(gdp_r, unemp_r)

    return {
        "gdp_forecast": gdp_f, "unemp_forecast": unemp_f,
        "gdp_realised": gdp_r, "unemp_realised": unemp_r,
        "M_hat": M_hat, "M_real": M_real
    }
```

---

### Macro Index Construction

We define a simple index combining standardised GDP and unemployment:

$$
M_t = 0.5 \, z(\text{GDP}_t) \;-\; 0.5 \, z(\text{UNEMP}_t)
$$

where $z(\cdot)$ is a z-score normalisation.

```{python}
def macro_index_from_gdp_unemp(GDP: np.ndarray, UNEMP: np.ndarray) -> np.ndarray:
    """Return M_t = 0.5*z(GDP) - 0.5*z(UNEMP)."""
    def z(x: np.ndarray) -> np.ndarray:
        mu = np.mean(x)
        sigma = np.std(x, ddof=0)
        sigma = sigma if sigma > 1e-12 else 1.0
        return (x - mu) / sigma
    return 0.5*z(GDP) - 0.5*z(UNEMP)
```

---

### Quick Demo

Generate and plot the three scenarios to see the difference.

```{python}
scenarios = ["baseline", "stress", "pandemic"]

fig, axes = plt.subplots(3, 2, figsize=(10, 8), sharex=True)
for i, sc in enumerate(scenarios):
    res = gen_macro_forecasts_and_realised(sc, T=20, rng=rng)
    axes[i,0].plot(res["gdp_forecast"], label="Forecast")
    axes[i,0].plot(res["gdp_realised"], label="Realised", alpha=0.7)
    axes[i,0].set_title(f"{sc.title()} GDP")
    axes[i,0].legend()
    axes[i,1].plot(res["unemp_forecast"], label="Forecast")
    axes[i,1].plot(res["unemp_realised"], label="Realised", alpha=0.7)
    axes[i,1].set_title(f"{sc.title()} Unemployment")
plt.tight_layout()
plt.show()
```


---

## 5) PIT Overlay (Point-in-Time Transitions)

We tilt the TTC matrix with a **logit-style overlay** driven by the macro index $M_t$.
The tilt strengths are in a $\beta$ matrix: positive entries push **downgrades/defaults** up in bad times (negative $M_t$), and **upgrades** are symmetric negatives.
After tilting, we **row-normalise** and keep **default (D)** absorbing.

```{python}
def _build_betas() -> np.ndarray:
    """Beta matrix β_ij with specified nearest-neighbour/default structure."""
    beta = np.zeros((4,4), dtype=float)

    # Downgrades / defaults (given)
    beta[IDX["A"], IDX["B"]] = 2.0
    beta[IDX["A"], IDX["C"]] = 2.5
    beta[IDX["A"], IDX["D"]] = 3.0

    beta[IDX["B"], IDX["C"]] = 1.5
    beta[IDX["B"], IDX["D"]] = 2.0

    beta[IDX["C"], IDX["D"]] = 1.2

    # Upgrades are negatives
    beta[IDX["B"], IDX["A"]] = -beta[IDX["A"], IDX["B"]]
    beta[IDX["C"], IDX["B"]] = -beta[IDX["B"], IDX["C"]]
    beta[IDX["C"], IDX["A"]] = -beta[IDX["A"], IDX["C"]]
    # Diagonals and other moves left at 0.0
    return beta


def pit_overlay(P_TTC: np.ndarray, M_t: float, betas: np.ndarray) -> np.ndarray:
    """Return a PIT transition matrix for the given macro index M_t."""
    P = np.array(P_TTC, dtype=float)
    # Logit-style tilt (elementwise)
    W = P * np.exp(betas * M_t)

    # Row normalisation; keep D absorbing
    for i in range(W.shape[0]):
        if i == IDX["D"]:
            W[i, :] = 0.0
            W[i, IDX["D"]] = 1.0
        else:
            s = W[i, :].sum()
            if s <= 0 or not np.isfinite(s):
                # Fallback to TTC row if degenerate
                W[i, :] = P[i, :]
            else:
                W[i, :] = W[i, :] / s
    return W
```

**Notes**

* $M_t$ is in **z-units** from your macro index; typical magnitudes are small, so `exp(betas*M_t)` remains well-behaved.
* We defensively fall back to TTC if a row becomes degenerate.

---

## 6) Kalman Filters (Naïve vs Anchored)

We compare a **naïve** KF (observations are the forecast index $M^{\hat{}}_t$) with an **anchored** KF that adds a **second observation** (“anchor”) that keeps the level close to a prior (0) with a tunable variance.
Before the anchoring date $T_{\text{anchor}}$, the anchor is **soft** (variance $\sigma_\*^2$); after it, the anchor becomes **hard** (very small variance).

```{python}
def kalman_naive(M_hat: np.ndarray, rho: float = 0.90, Q: float = None, R: float = 0.25) -> np.ndarray:
    """Naïve KF: state m_t is AR(1); observation y_t = M_hat[t]."""
    T = len(M_hat)
    if Q is None:
        Q = 1 - rho**2  # keeps stationary variance ≈1

    # State init
    m = 0.0
    P = 1.0
    H = 1.0

    m_filt = np.zeros(T)
    for t in range(T):
        # Predict
        m_pred = rho * m
        P_pred = rho * P * rho + Q

        # Update with y_t = M_hat[t]
        y = M_hat[t]
        S = H * P_pred * H + R          # scalar
        K = P_pred * H / S               # scalar
        m = m_pred + K * (y - H * m_pred)
        P = (1 - K * H) * P_pred

        m_filt[t] = m
    return m_filt
```


```{python}
def kalman_anchored(
    M_hat: np.ndarray,
    T_anchor: int = 20,
    rho: float = 0.90,
    Q: float = None,
    R: float = 0.25,
    sigma_star2_pre: float = 0.25
) -> np.ndarray:
    """
    Anchored KF with stacked observation y_t = [M_hat_t, 0]^T, H = [[1],[1]].
    - Before T_anchor: soft anchor with variance sigma_star2_pre.
    - On/after T_anchor: hard anchor (very small variance).
    """
    T = len(M_hat)
    if Q is None:
        Q = 1 - rho**2

    m = 0.0
    P = 1.0

    H = np.array([[1.0],[1.0]])  # (2,1)
    m_filt = np.zeros(T)

    for t in range(T):
        # Predict
        m_pred = rho * m
        P_pred = rho * P * rho + Q

        # Stacked obs
        y = np.array([M_hat[t], 0.0])        # (2,)
        if t < T_anchor:
            R_aug = np.diag([R, sigma_star2_pre])
        else:
            R_aug = np.diag([R, 1e-12])      # hard anchor

        # Innovation covariance: S = H P_pred H^T + R
        # Use solves for stability (avoid explicit inverse)
        S = H @ np.array([[P_pred]]) @ H.T + R_aug        # (2,2)
        # K = P_pred H^T S^{-1}
        K = (np.array([[P_pred]]) @ H.T) @ np.linalg.solve(S, np.eye(2))  # (1,2)

        # Innovation: y - H m_pred
        innov = y - (H[:, 0] * m_pred)       # (2,)
        m = m_pred + (K @ innov)[0]          # scalar
        P = (1.0 - (K @ H)[0, 0]) * P_pred   # scalar

        m_filt[t] = m
    return m_filt
```

**Tips**

* If you’re on Python 3.8, you don’t need generics (e.g. `tuple[...]`); these functions avoid them.
* If you ever see type-subscript errors again, either switch to `typing.Tuple[...]` or pin Python 3.11+ with a `runtime.txt` (`python-3.11`) for Binder/Thebe.

---

### Quick demo: compare naïve vs anchored KF

This cell assumes you’ve already generated a macro scenario and have its **forecast index** `M_hat` (e.g., from Section 4).
It plots both filtered state paths side by side.

```{python}
# Example: use a baseline scenario's forecast macro index
betas = _build_betas()  # not used here, but ensures PIT section is executed

demo = gen_macro_forecasts_and_realised("baseline", T=40, rng=rng)
Mhat_demo = demo["M_hat"]

m_naive = kalman_naive(Mhat_demo, rho=0.9, R=0.25)
m_anchor = kalman_anchored(Mhat_demo, T_anchor=20, rho=0.9, R=0.25, sigma_star2_pre=0.25)

fig, ax = plt.subplots(figsize=(8, 4))
ax.plot(Mhat_demo, label="Forecast index  $M^\\hat{}_t$", alpha=0.6)
ax.plot(m_naive, label="Naïve KF  $m_t$")
ax.plot(m_anchor, label="Anchored KF  $m_t$")
ax.axvline(20, ls="--", alpha=0.5, label="Anchor switch (T_anchor)")
ax.set_title("Naïve vs Anchored Kalman Filter (Baseline scenario)")
ax.legend()
plt.tight_layout()
plt.show()
```


---

## 7) Propagation

We propagate an initial cross-sectional distribution $\pi_0$ through a **time-varying** sequence of transition matrices $\{P_t\}_{t=1}^T$. We also extract the **default mass** series $Y_t = \pi_t[D]$.

```{python}
from typing import List, Dict, Tuple  # 3.8-safe generics

def propagate_distribution(pi0: np.ndarray, P_ts: List[np.ndarray]) -> np.ndarray:
    """Return array of shape (T+1, 4) of distributions over time."""
    T = len(P_ts)
    pi = np.zeros((T+1, len(RATINGS)))
    pi[0, :] = pi0
    current = pi0.copy()
    for t in range(T):
        current = current @ P_ts[t]
        pi[t+1, :] = current
    return pi

def compute_pd_series(pi_ts: np.ndarray) -> np.ndarray:
    """Return Y_t = probability mass in default at each t."""
    return pi_ts[:, IDX["D"]]
```

---

## 8) Experiment Runner

This wraps everything: scenario → macro paths → filtering method → PIT matrices → PD term structure.
It also **saves CSVs and a figure** to your local `outputs/` directory.

```{python}
def run_experiment(
    scenario: str,
    method: str,
    N: int = N_DEFAULT,
    T: int = T_DEFAULT,
    seed: int = 12345
) -> Dict[str, object]:
    """Run scenario × method. Save CSVs and figures. Return key arrays + file paths."""
    rng = set_seed(seed + hash((scenario, method)) % 10_000)  # slight offset per run

    # Inputs
    pi0 = np.array([0.45, 0.40, 0.15, 0.00], dtype=float)
    labels0 = gen_initial_portfolio(N, pi0, rng=rng)
    P_TTC = build_P_TTC()
    betas = _build_betas()

    # Macro
    macro = gen_macro_forecasts_and_realised(scenario, T=T, rng=rng)
    M_hat = macro["M_hat"]
    M_real = macro["M_real"]

    # Filtering method selection
    if method == "raw":
        M_est = M_real.copy()
        M_eff = M_est.copy()
    elif method == "naive":
        M_est = kalman_naive(M_hat, rho=0.90, Q=None, R=0.25)
        M_eff = M_est.copy()
    elif method == "anchored":
        M_est = kalman_anchored(M_hat, T_anchor=T, rho=0.90, Q=None, R=0.25, sigma_star2_pre=0.25)
        M_eff = M_est.copy()
    else:
        raise ValueError("Unknown method")

    # Time-varying PIT matrices
    P_ts = [pit_overlay(P_TTC, float(M_eff[t]), betas) for t in range(T)]

    # Propagate
    pi_ts = propagate_distribution(pi0, P_ts)
    Y_t = compute_pd_series(pi_ts)

    # TTC baseline path (neutral M=0)
    P_neutral = [P_TTC.copy() for _ in range(T)]
    pi_ttc = propagate_distribution(pi0, P_neutral)
    Y_ttc = compute_pd_series(pi_ttc)

    # Save transition matrices
    tm_rows = []
    for t in range(T):
        for i, ri in enumerate(RATINGS):
            for j, rj in enumerate(RATINGS):
                tm_rows.append({"t": t+1, "from": ri, "to": rj, "P": P_ts[t][i, j]})
    df_tm = pd.DataFrame(tm_rows)
    f_tm = OUTDIR / f"transition_matrices_{scenario}_{method}.csv"
    df_tm.to_csv(f_tm, index=False)

    # Save macro paths
    df_macro = pd.DataFrame({
        "t": np.arange(1, T+1),
        "M_forecast": M_hat,
        "M_realised": M_real,
        "M_estimate": M_est
    })
    f_macro = OUTDIR / f"macro_paths_{scenario}_{method}.csv"
    df_macro.to_csv(f_macro, index=False)

    # Save PD term structures
    df_pd = pd.DataFrame({
        "t": np.arange(0, T+1),
        "Y_t": Y_t,
        "Y_ttc": Y_ttc
    })
    f_pd = OUTDIR / f"pd_term_structures_{scenario}_{method}.csv"
    df_pd.to_csv(f_pd, index=False)

    # Figure: macro filter
    fig1 = plt.figure()
    plt.plot(np.arange(1, T+1), M_hat, label="Forecast (M̂)")
    plt.plot(np.arange(1, T+1), M_real, label="Realised M")
    plt.plot(np.arange(1, T+1), M_est, label=f"Estimate: {method}")
    plt.axhline(0.0)
    plt.xlabel("Quarter")
    plt.ylabel("Macro index (z)")
    plt.title(f"Macro filter: {scenario} × {method}")
    plt.legend()
    f_fig1 = OUTDIR / f"macro_filter_{scenario}_{method}.png"
    fig1.savefig(f_fig1, bbox_inches="tight")
    plt.close(fig1)

    return {
        "labels0": labels0,
        "P_ts": P_ts,
        "pi_ts": pi_ts,
        "Y_t": Y_t,
        "Y_ttc": Y_ttc,
        "M_hat": M_hat,
        "M_real": M_real,
        "M_est": M_est,
        "files": {
            "transition_matrices": str(f_tm),
            "macro_paths": str(f_macro),
            "pd_term_structures": str(f_pd),
            "macro_figure": str(f_fig1),
        },
    }
```

---

## 9) PD Bands and Metrics

We plot **PD term-structure bands** across scenarios for a fixed method, compute **cross-scenario variance** of $Y_t$, and estimate **loss volatility** via simple macro-noise Monte Carlo.

```{python}
def _pd_bands_figure(results_per_scn: Dict[str, Dict[str, object]], method: str, T: int = T_DEFAULT) -> str:
    """Create PD bands plot per method across scenarios. Return file path."""
    tgrid = np.arange(0, T+1)
    fig = plt.figure()
    # TTC baseline from first scenario's output
    any_key = next(iter(results_per_scn))
    Y_ttc = results_per_scn[any_key]["Y_ttc"]
    plt.plot(tgrid, Y_ttc, label="TTC baseline")
    # Scenario paths
    for scn, res in results_per_scn.items():
        plt.plot(tgrid, res["Y_t"], label=f"{scn}")
    plt.xlabel("Quarter")
    plt.ylabel("Cumulative PD (π_t[D])")
    plt.title(f"PD term-structure bands across scenarios — {method}")
    plt.legend()
    fpath = OUTDIR / f"pd_bands_{method}.png"
    fig.savefig(fpath, bbox_inches="tight")
    plt.close(fig)
    return str(fpath)

def variance_across_scenarios(Y_by_scn: Dict[str, np.ndarray]) -> pd.DataFrame:
    """Variance of Y_t across scenarios for each horizon."""
    # Y_by_scn: {scenario: Y_t array}
    Ys = np.stack([v for v in Y_by_scn.values()], axis=0)  # (n_scn, T+1)
    var_t = Ys.var(axis=0, ddof=0)
    df = pd.DataFrame({
        "t": np.arange(Ys.shape[1]),
        "var_Y_t": var_t
    })
    return df

def monte_carlo_loss_volatility(
    scenario: str,
    method: str,
    pi0: np.ndarray,
    P_TTC: np.ndarray,
    betas: np.ndarray,
    base_forecasts: Dict[str, np.ndarray],
    n_rep: int = 200,
    seed: int = 777
) -> Dict[str, float]:
    """Std of final loss fraction Y_T across forecast-noise replications."""
    rng = np.random.default_rng(seed + hash((scenario, method)) % 10_000)
    T = len(base_forecasts["M_hat"])
    YT = np.zeros(n_rep)
    for r in range(n_rep):
        # Realised from forecast + noise (+ pandemic rebound if relevant)
        gdp_f = base_forecasts["gdp_forecast"]
        un_f = base_forecasts["unemp_forecast"]
        gdp_r = gdp_f + rng.normal(0.0, 0.2, size=T)
        un_r = un_f + rng.normal(0.0, 0.2, size=T)
        if scenario.lower().startswith("pandemic") and T >= 4:
            gdp_r[3] += 2.5
            un_r[3] -= 0.8
        M_hat = macro_index_from_gdp_unemp(gdp_f, un_f)
        M_real = macro_index_from_gdp_unemp(gdp_r, un_r)

        # Method
        if method == "raw":
            M_eff = M_real
        elif method == "naive":
            M_eff = kalman_naive(M_hat, rho=0.90, Q=None, R=0.25)
        elif method == "anchored":
            M_eff = kalman_anchored(M_hat, T_anchor=T, rho=0.90, Q=None, R=0.25, sigma_star2_pre=0.25)
        else:
            raise ValueError("Unknown method")

        P_ts = [pit_overlay(P_TTC, float(M_eff[t]), betas) for t in range(T)]
        pi_ts = propagate_distribution(pi0, P_ts)
        YT[r] = pi_ts[-1, IDX["D"]]
    return {"mean": float(YT.mean()), "std": float(YT.std(ddof=0))}
```

---

### Quick end-to-end demo

This runs all three scenarios with the **anchored** method, creates the PD bands chart, prints variance across scenarios, and shows where files were saved.

```{python}
scenarios = ["baseline", "stress", "pandemic"]
method = "anchored"
T_demo = 20

results = {sc: run_experiment(sc, method=method, T=T_demo, seed=2025) for sc in scenarios}

# PD bands figure
band_path = _pd_bands_figure(results, method=method, T=T_demo)

# Variance across scenarios
var_df = variance_across_scenarios({sc: res["Y_t"] for sc, res in results.items()})
display(var_df.head(8))

print("Saved files:")
for sc, res in results.items():
    print(f"  [{sc}] macro: {res['files']['macro_paths']}")
    print(f"  [{sc}] PDs  : {res['files']['pd_term_structures']}")
print(f"  Bands figure: {band_path}")
```

---



## 10) Main (reproducible end-to-end run)

This orchestrates everything: runs all **scenarios × methods**, saves outputs (CSVs + PNGs) under `outputs/`, creates a file registry, and prints a couple of table heads for a quick sanity check.

```{python}
import zlib  # stable seed offsets

def _stable_offset(scenario: str, method: str, mod: int = 10_000) -> int:
    """Deterministic offset for seeds (independent of Python hash randomization)."""
    key = f"{scenario}|{method}".encode()
    return zlib.crc32(key) % mod

def main():
    seed = 20250821
    rng = set_seed(seed)

    scenarios = ["baseline", "stress", "pandemic"]
    methods = ["raw", "naive", "anchored"]
    pi0 = np.array([0.45, 0.40, 0.15, 0.00], dtype=float)
    P_TTC = build_P_TTC()
    betas = _build_betas()

    # Run experiments
    all_results = {m: {} for m in methods}
    file_registry = []

    for m in methods:
        for scn in scenarios:
            # Stable, per-run offset for determinism across environments
            res = run_experiment(
                scn, m,
                N=N_DEFAULT,
                T=T_DEFAULT,
                seed=seed + _stable_offset(scn, m)
            )
            all_results[m][scn] = res
            files = res["files"]
            file_registry.append({
                "scenario": scn,
                "method": m,
                **files
            })

    # PD bands per method and variance across scenarios
    var_tables = []
    pd_band_files = []
    for m in methods:
        # Bands figure
        f = _pd_bands_figure(all_results[m], m, T=T_DEFAULT)
        pd_band_files.append({"method": m, "pd_bands_figure": f})

        # Variance table
        Y_by_scn = {scn: all_results[m][scn]["Y_t"] for scn in scenarios}
        df_var = variance_across_scenarios(Y_by_scn)
        df_var.insert(0, "method", m)
        var_tables.append(df_var)

    df_var_all = pd.concat(var_tables, ignore_index=True)
    f_var = OUTDIR / "variance_Y_across_scenarios.csv"
    df_var_all.to_csv(f_var, index=False)

    # Monte Carlo lifetime loss volatility per scenario × method
    mc_rows = []
    for m in methods:
        for scn in scenarios:
            base_forecasts = gen_macro_forecasts_and_realised(scn, T=T_DEFAULT, rng=rng)
            mc = monte_carlo_loss_volatility(
                scn, m, pi0, P_TTC, betas, base_forecasts,
                n_rep=200, seed=seed + _stable_offset(scn, m)
            )
            mc_rows.append({"scenario": scn, "method": m, "YT_mean": mc["mean"], "YT_std": mc["std"]})
    df_mc = pd.DataFrame(mc_rows)
    f_mc = OUTDIR / "lifetime_loss_volatility_mc.csv"
    df_mc.to_csv(f_mc, index=False)

    # Summary metrics file combining tables
    df_summary = pd.DataFrame({
        "file": [
            str(f_var),
            str(f_mc),
        ],
        "description": [
            "Variance of Y_t across scenarios by horizon and method",
            "Lifetime loss volatility (std of Y_T) across 200 MC reps per scenario×method",
        ]
    })
    f_summary = OUTDIR / "summary_metrics.csv"
    df_summary.to_csv(f_summary, index=False)

    # Append band figures to registry and write a registry CSV
    for item in pd_band_files:
        file_registry.append({
            "scenario": "all",
            "method": item["method"],
            "macro_paths": "",
            "transition_matrices": "",
            "pd_term_structures": "",
            "macro_figure": "",
            "pd_bands_figure": item["pd_bands_figure"]
        })
    df_registry = pd.DataFrame(file_registry)
    f_registry = OUTDIR / "file_registry.csv"
    df_registry.to_csv(f_registry, index=False)

    # Return a small dict of paths for printing
    return {
        "registry": str(f_registry),
        "variance_table": str(f_var),
        "mc_table": str(f_mc),
        "summary": str(f_summary),
        "example_macro": all_results["naive"]["baseline"]["files"]["macro_paths"],
        "example_pd": all_results["anchored"]["pandemic"]["files"]["pd_term_structures"],
    }

# Execute main and print heads
paths = main()

print("Saved files (selected):")
for k, v in paths.items():
    print(f" - {k}: {v}")

print("\nExample heads:")
df_macro = pd.read_csv(paths["example_macro"])
df_pd = pd.read_csv(paths["example_pd"])

print("\nmacro_paths (baseline × naive) head:")
print(df_macro.head(6))

print("\npd_term_structures (pandemic × anchored) head:")
print(df_pd.head(6))
```

**Why the `crc32` change matters**
Using `hash((scenario, method))` makes seeds differ across runs (Python salts `hash()` for security).
`zlib.crc32` on a string key is **stable everywhere**, so your seeds — and outputs — are truly reproducible.




# 10B) PD Bands — show inline + save

We plot **PD term-structure bands** across scenarios for a fixed method.
This block assumes you have `results = {scenario: run_experiment(...)}`
or you run the “Populate results” helper below.

### 10B.1 Populate results (if not already in memory)

```{python}
# If you already ran main() and have results, skip this.
scenarios = ["baseline", "stress", "pandemic"]
method = "anchored"
T_demo = 20

results = {sc: run_experiment(sc, method=method, T=T_demo, seed=2025) for sc in scenarios}
```

### 10B.2 Plot PD bands (inline + saved)

```{python}
# Inline + save version of PD bands
tgrid = np.arange(0, T_demo+1)
fig, ax = plt.subplots(figsize=(8, 4))

# TTC baseline from first scenario
any_key = next(iter(results))
Y_ttc = results[any_key]["Y_ttc"]
ax.plot(tgrid, Y_ttc, label="TTC baseline")

# Scenario paths
for scn, res in results.items():
    ax.plot(tgrid, res["Y_t"], label=scn)

ax.set_xlabel("Quarter")
ax.set_ylabel("Cumulative PD (π_t[D])")
ax.set_title(f"PD term-structure bands across scenarios — {method}")
ax.legend()

f_bands = OUTDIR / f"pd_bands_{method}_inline.png"
fig.savefig(f_bands, bbox_inches="tight")
plt.show()
f_bands
```

*What to look for:* Anchored should usually sit tighter than raw/naive under stressed scenarios.

---

# 11) Appendix Figures — MC Distributions & Summaries

We’ll (1) ensure per-scenario MC **sample CSVs** exist, then (2) build four multi-panel figures.
All plots display inline **and** are saved to `outputs/`.

### 11.0 Ensure/Generate MC sample CSVs

```{python}
from typing import Dict
import zlib

def _stable_offset_key(*parts: str, mod: int = 10_000) -> int:
    """Deterministic small integer from strings (cross-env stable)."""
    key = "|".join(parts).encode()
    return zlib.crc32(key) % mod

def _ensure_mc_samples_csvs(
    outdir: Path,
    scenarios = ("baseline", "stress", "pandemic"),
    methods = ("raw", "naive", "anchored"),
    n_rep: int = 200,
    seed_base: int = 2025,
    T: int = T_DEFAULT,
    pi0: np.ndarray = np.array([0.45, 0.40, 0.15, 0.00], dtype=float),
):
    """Create mc_samples_YT_{scenario}.csv if missing (columns: one per method, n_rep rows)."""
    outdir.mkdir(parents=True, exist_ok=True)
    P_TTC = build_P_TTC()
    betas = _build_betas()

    for scn in scenarios:
        f_samples = outdir / f"mc_samples_YT_{scn}.csv"
        if f_samples.exists():
            continue

        # deterministic base forecasts
        rng_base = set_seed(seed_base + _stable_offset_key("base", scn))
        base_forecasts = gen_macro_forecasts_and_realised(scn, T=T, rng=rng_base)

        data: Dict[str, np.ndarray] = {}
        for m in methods:
            mc = []
            rng_mc_seed = seed_base + _stable_offset_key("mc", scn, m)
            rng_mc = np.random.default_rng(rng_mc_seed)

            for r in range(n_rep):
                gdp_f = base_forecasts["gdp_forecast"]
                un_f  = base_forecasts["unemp_forecast"]
                gdp_r = gdp_f + rng_mc.normal(0.0, 0.2, size=T)
                un_r  = un_f + rng_mc.normal(0.0, 0.2, size=T)
                if scn.lower().startswith("pandemic") and T >= 4:
                    gdp_r[3] += 2.5
                    un_r[3]  -= 0.8

                M_hat  = macro_index_from_gdp_unemp(gdp_f, un_f)
                M_real = macro_index_from_gdp_unemp(gdp_r, un_r)

                if m == "raw":
                    M_eff = M_real
                elif m == "naive":
                    M_eff = kalman_naive(M_hat, rho=0.90, Q=None, R=0.25)
                elif m == "anchored":
                    M_eff = kalman_anchored(M_hat, T_anchor=T, rho=0.90, Q=None, R=0.25, sigma_star2_pre=0.25)
                else:
                    raise ValueError("Unknown method")

                P_ts = [pit_overlay(P_TTC, float(M_eff[t]), betas) for t in range(T)]
                pi_ts = propagate_distribution(pi0, P_ts)
                mc.append(pi_ts[-1, IDX["D"]])  # Y_T

            data[m] = np.array(mc, dtype=float)

        pd.DataFrame(data).to_csv(f_samples, index=False)

# ensure data
_ensure_mc_samples_csvs(OUTDIR)
```

---

### 11.1 Notched boxplots (inline + saved)

```{python}
scenarios = ["baseline", "stress", "pandemic"]
methods   = ["raw", "naive", "anchored"]

fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)
for ax, scn in zip(axes, scenarios):
    df = pd.read_csv(OUTDIR / f"mc_samples_YT_{scn}.csv")
    ax.boxplot([df[m] for m in methods], labels=methods, notch=True, showfliers=False)
    ax.set_title(scn.capitalize()); ax.set_xlabel("Method")
axes[0].set_ylabel("Lifetime PD at T")
fig.suptitle("Notched boxplots — Lifetime PD distributions (200 MC reps)", y=1.02)

f_box = OUTDIR / "combined_notched_boxplots_all.png"
fig.savefig(f_box, bbox_inches="tight")
plt.show()
f_box
```

*What to look for:* medians and notch overlaps; anchored often narrows dispersion.

---

### 11.2 Violin plots (inline + saved)

```{python}
fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)
for ax, scn in zip(axes, scenarios):
    df = pd.read_csv(OUTDIR / f"mc_samples_YT_{scn}.csv")
    ax.violinplot([df[m] for m in methods], showmeans=True, showmedians=True)
    ax.set_xticks(range(1, len(methods)+1)); ax.set_xticklabels(methods)
    ax.set_title(scn.capitalize()); ax.set_xlabel("Method")
axes[0].set_ylabel("Lifetime PD at T")
fig.suptitle("Violin plots — Lifetime PD distributions (200 MC reps)", y=1.02)

f_viol = OUTDIR / "combined_violins_all.png"
fig.savefig(f_viol, bbox_inches="tight")
plt.show()
f_viol
```

*What to look for:* distribution shape and tails by method.

---

### 11.3 Jittered box+dot (inline + saved; deterministic jitter)

```{python}
rng_jitter = np.random.default_rng(12345)
fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)
for ax, scn in zip(axes, scenarios):
    df = pd.read_csv(OUTDIR / f"mc_samples_YT_{scn}.csv")
    ax.boxplot([df[m] for m in methods], labels=methods, showfliers=False)
    for i, m in enumerate(methods, start=1):
        y = df[m].values
        x = rng_jitter.normal(i, 0.06, size=len(y))  # fixed jitter
        ax.plot(x, y, linestyle="", marker="o", alpha=0.35, markersize=3)
    ax.set_title(scn.capitalize()); ax.set_xlabel("Method")
axes[0].set_ylabel("Lifetime PD at T")
fig.suptitle("Jittered box+dot — Lifetime PD distributions (200 MC reps)", y=1.02)

f_boxdot = OUTDIR / "combined_boxdot_all.png"
fig.savefig(f_boxdot, bbox_inches="tight")
plt.show()
f_boxdot
```

*What to look for:* outliers and spread around the box.

---

### 11.4 Mean ± std bars from summary table (inline + saved)

```{python}
df_mc = pd.read_csv(OUTDIR / "lifetime_loss_volatility_mc.csv")

fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)
for ax, scn in zip(axes, scenarios):
    sub = df_mc[df_mc["scenario"] == scn].copy()
    order = ["raw", "naive", "anchored"]
    sub["method"] = pd.Categorical(sub["method"], categories=order, ordered=True)
    sub = sub.sort_values("method")
    x = np.arange(len(order))
    ax.bar(x, sub["YT_mean"].values, yerr=sub["YT_std"].values, capsize=4)
    ax.set_xticks(x); ax.set_xticklabels(order)
    ax.set_title(scn.capitalize()); ax.set_xlabel("Method")
axes[0].set_ylabel("Lifetime PD at T (mean ± std)")
fig.suptitle("Lifetime PD (200 MC reps) — mean with volatility", y=1.02)

f_bars = OUTDIR / "combined_mc_lifetime_pd_bars_all.png"
fig.savefig(f_bars, bbox_inches="tight")
plt.show()
f_bars
```

*What to look for:* which method lowers mean and/or volatility under each scenario.

---

### 11.5 (Optional) Tiny gallery summary

```{python}
print("Saved appendix figures:")
print(" - Notched boxplots :", f_box)
print(" - Violin plots     :", f_viol)
print(" - Jittered box+dot :", f_boxdot)
print(" - Mean±Std bars    :", f_bars)
```

---



