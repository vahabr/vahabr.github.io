[
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "My teaching experience spans graduate-level courses in optimization, control systems, and applied mathematics at both Delft University of Technology and the University of Groningen. I have contributed as instructor and teaching assistant in lectures, tutorials, and project-based assignments, with responsibilities ranging from exercise design and student supervision to grading and symposium organisation. These activities allowed me to combine theoretical insights with hands-on problem solving, while engaging closely with MSc students in engineering and systems control.\nIn addition to my past university teaching, I am developing a series of online courses in my areas of expertise — with especial focus on quantitative and financial risk and algorithmic tradings. These courses will soon be available to a broader audience interested in applying rigorous methods to complex, uncertain systems."
  },
  {
    "objectID": "teaching.html#online-courses",
    "href": "teaching.html#online-courses",
    "title": "Teaching",
    "section": "Online Courses",
    "text": "Online Courses\n\nUnder Constructions"
  },
  {
    "objectID": "teaching.html#university-of-groningen",
    "href": "teaching.html#university-of-groningen",
    "title": "Teaching",
    "section": "University of Groningen",
    "text": "University of Groningen\n\nSep 2018 — Dec 2019\n\n4th Quarter, 2019 | Stochastic Programming\nThis course was part of MSc program in Industrial Engineering at Engineering and Technology Institute Groningen, University of Groningen. The course introduced students to methods for modelling and solving optimisation problems under uncertainty, with emphasis on scenario-based formulations and decomposition techniques. I contributed by delivering lectures, guiding students through problem-solving sessions, and designing assignments that connected theory to practical applications in engineering and operations research.\n\n\n2nd Quarter, 2018 | Mechatronics\nThis course was part of MSc program in Industrial Engineering at Engineering and Technology Institute Groningen, University of Groningen. The course covered modelling, analysis, and control of mechatronic systems, combining mechanical, electronic, and computer engineering aspects.\nMy role included supervising project-based exercises, assisting students in integrating hardware and software, and providing feedback on their design and implementation approaches."
  },
  {
    "objectID": "teaching.html#delft-university-of-technology",
    "href": "teaching.html#delft-university-of-technology",
    "title": "Teaching",
    "section": "Delft University of Technology",
    "text": "Delft University of Technology\n\nSep 2014 — Aug 2018\n\n4th Quarter, 2018 | Networked and Distributed Control Systems (SC42100)\nThis course was part of MSc program in Systems and Control Engineering at Delft Center for Systems and Control (DCSC) Institute, Delft University of Technology. I served as co-instructor of this course. I was responsible for the exercise lessons and a tutorial on Linear Matrix Inequalities (LMI) with Semidefinite Programming (SDP).\nThe course started with the modelling of networked control systems (NCS) and then addressed distributed control of such systems.\n\n\n4th Quarter, 2017 | Networked and Distributed Control Systems (SC42100)\nThis course was part of MSc program in Systems and Control Engineering at Delft Center for Systems and Control (DCSC) Institute, Delft University of Technology. I served as co-instructor of this course. I was responsible for the exercise lessons and a tutorial on Linear Matrix Inequalities (LMI) with Semidefinite Programming (SDP).\nThe course introduced networked control systems (NCS) and focused on distributed control problems in interconnected subsystems.\n\n\n3rd Quarter, 2016 | Knowledge-Based Control Systems (SC4081)\nThis course was part of MSc program in Systems and Control Engineering at Delft Center for Systems and Control (DCSC) Institute, Delft University of Technology. I served as a teaching assistant for this course. My primary responsibility was the Literature Assignment, where students produced a paper, presentation, and four peer review forms that together counted for 20% of the final grade.\nI managed student questions in the relevant topics, graded their work, and organised the final symposium where students presented their results.\n\n\n3rd Quarter, 2015 | Knowledge-Based Control Systems (SC4081)\nThis course was part of MSc program in Systems and Control Engineering at Delft Center for Systems and Control (DCSC) Institute, Delft University of Technology. I served as a teaching assistant for this course. My primary responsibility was the Literature Assignment, where students produced a paper, presentation, and four peer review forms that together counted for 20% of the final grade.\nI managed student questions, graded assignments, and organised the final symposium where students presented their results."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "This page presents a selection of my research outputs, spanning journal articles, conference papers, books, and theses. The works are grouped by category and year, with the most recent contributions listed first. My publications reflect ongoing interests in stochastic modelling, optimisation, control of complex systems, and quantitative risk management, with applications across energy, infrastructure, and finance."
  },
  {
    "objectID": "publications.html#in-preprations",
    "href": "publications.html#in-preprations",
    "title": "Publications",
    "section": "In Preprations",
    "text": "In Preprations\n\nV. Rostampour. Book Project: Quantitative Interview Questions (QIQ V1.0)\nV. Rostampour. Stabilising Lifetime PD Models under Forecast Uncertainty. In Preparation to submit for Journal of Credit Risk\nV. Rostampour, Akiko Takeda, Takafumi Kanamori. Vanishing Duality Gap in Uncertain Nonconvex Problems. In Preparation to submit for SIAM Journal on Optimization (SIOPT)"
  },
  {
    "objectID": "publications.html#journal-articles",
    "href": "publications.html#journal-articles",
    "title": "Publications",
    "section": "Journal Articles",
    "text": "Journal Articles\n\n2021\n\nV. Rostampour, T. Keviczky. Distributed Stochastic MPC for Large-Scale Systems with Private and Common Uncertainty Sources. Energies, 14(1), 23, 2021.\n\n\n\n2020\n\nV. Rostampour, T. Keviczky. Demand Flexibility Management for Buildings-to-Grid Integration with Uncertain Generation. Energies, 13(24):6532, 2020.\nV. Rostampour, R. Ferrari, A. H. Teixeira, T. Keviczky. Privatized Distributed Anomaly Detection for Large-Scale Nonlinear Uncertain Systems. IEEE Transactions on Automatic Control (Full Paper), 2020.\n\n\n\n2019\n\nZ. Sun, V. Rostampour, M. Cao. Self-Triggered Stochastic MPC for Linear Systems with Disturbances. IEEE Control Systems Letters, Vol. 3, Issue. 4, Pp. 787-792, 2019.\nV. Rostampour, M. Jaxa-Rozen , M. Bloemendal , J. Kwakkel, and T. Keviczky. ATES Smart Grids: Large-Scale Seasonal Energy Storage as A Distributed Energy Management Solution. Applied Energy, Vol. 242, Pp. 624-639, 2019.\n\n\n\n2018\n\nV. Rostampour, O. t. Haar, T. Keviczky. Distributed Stochastic Reserve Scheduling in AC Power Systems With Uncertain Generation. IEEE Transactions on Power Systems, Vol. 34, Pp. 1005-1020, 2018.\nV. Rostampour, T. Keviczky . Probabilistic Energy Management for Building Climate Comfort in Smart Thermal Grids with Seasonal Storage Systems. IEEE Transactions on Smart Grid, Vol. 10, Pp. 3687-3697, 2018.\n\n\n\n2016\n\nV. Rostampour, M. Jaxa-Rozen, M. Bloemendal, T. Keviczky. Building Climate Energy Management in Smart Thermal Grids via Aquifer Thermal Energy Storage Systems. Energy Procedia, Vol. 97, Pp. 59-66, 2016."
  },
  {
    "objectID": "publications.html#conference-papers",
    "href": "publications.html#conference-papers",
    "title": "Publications",
    "section": "Conference Papers",
    "text": "Conference Papers\n\n2020\n\nJ. Venkatasubramanian, V. Rostampour, T.S. Badings, T. Keviczky. Stochastic MPC for Energy Management in Smart Grids with CVaR as Penalty Function. IEEE PES Innovative Smart Grid Technologies Europe (ISGT-Europe), 309-313, 2020\n\n\n\n2019\n\nV. Rostampour, T.S. Badings, J.M.A. Scherpen. Buildings-to-Grid Integration with High Wind Power Penetration. Conference on Decision and Control (CDC), 2976-2981, Nice, France, December 2019\nT.S. Badings, V. Rostampour, J.M.A. Scherpen. Distributed Building Energy Storage Units for Frequency Control Service in Power Systems. IFAC Smart Grid and Renewable Energy Systems. Vol. 52, Issue. 4, Pp.228-233 , Jeju, Korea. June , 2019\n\n\n\n2018\n\nS.Boersma, V. Rostampour, B.Doekemeijer, J.W. van Wingerden, T. Keviczky. A Model Predictive Wind Farm Controller with Linear Parameter-Varying Models. IFAC Conference on Nonlinear Model Predictive Control. Madison, Wisconsin, US. August, 2018\nV. Rostampour, R. Ferrari, A. Teixeira, T. Keviczky — Paul M. Frank Award (finalist) — Differentially-Private Distributed Fault Diagnosis for Large-Scale Nonlinear Uncertain Systems. IFAC SAFEPROCESS, Vol. 51, Issue 24, Pp. 975-982, Warsaw, Poland. August, 2018\nS.Boersma, V. Rostampour, B.Doekemeijer, J.W. van Wingerden. A Constrained Model Predictive Wind Farm Controller Providing Active Power Control: LES Study. The Science of Making Torque from Wind (TORQUE), Physics. Milan, Italy. March, 2018\nV. Rostampour, T. Keviczky. Distributed Stochastic Model Predictive Control Synthesis for Large-Scale Uncertain Linear Systems. American Control Conference (ACC), June 27–29, Pp. 2071-2077. Milwaukee, WI, USA. June, 2018\n\n\n\n2017\n\nV. Rostampour, O. t. Haar, T. Keviczky. Tractable Reserve Scheduling in AC Power Systems With Uncertain Wind Power Generation Conference on Decision and Control (CDC), Pp. 2647-2654. Melbourne, Australia. December, 2017\nV. Rostampour, T. Keviczky. Energy Management for Building Climate Comfort in Uncertain Smart Thermal Grids with ATES. IFAC World Congress. vol. 50, no.1, pp. 13698-13705. Toulouse, France. July, 2017\nV. Rostampour, R. Ferrari, T. Keviczky. A Set Based Probabilistic Approach to Threshold Design for Optimal Fault Detection. American Control Conference (ACC), Pp. 5422-5429. Seattle, US. June, 2017\nV. Rostampour, M. Bloemendal, T. Keviczky. MPC of GSHP coupled with ATES System in Heating and Cooling Networks of a Building. International Energy Agency (IEA) Conference on Heat Pump.Rotterdam, The Netherlands. March, 2017\nM. Bloemendal, M. Jaxa-Rozen, V. Rostampour. Use it or Lose it: Adaptive Governance of Aquifers with ATES. International Energy Agency (IEA) Conference on Heat Pump. Rotterdam, The Netherlands. March, 2017\n\n\n\n2016\n\nV. Rostampour, Dieky Adzkiya, Sadegh Soudjani, Bart De Schutter, T. Keviczky. Chance Constrained Model Predictive Controller Synthesis for Stochastic Max-Plus Linear Systems. Systems, Man, and Cybernetics (SMC), Pp. 3581-3588. Budapest, Hungry. October, 2016\nV. Rostampour, M. Bloemendal, M. Jaxa-Rozen, T. Keviczky. A Control-Oriented Model For Combined Building Climate Comfort and Aquifer Thermal Energy Storage System. European Geothermal Congress (EGC). Strasburg, France. September, 2016\nM. Jaxa-Rozen, M. Bloemendal, V. Rostampour, J. Kwakkel. Assessing the sustainable application of Aquifer Thermal Energy Storage. European Geothermal Congress (EGC). Strasburg, France. September, 2016\nV. Rostampour, T. Keviczky. Robust Randomized Model Predictive Control for Energy Balance in Smart Thermal Grids. European Control Conference (ECC), Pp. 1201-1208. Aalborg, Denmark. June 2016\n\n\n\n2015\n\nV. Rostampour, P. Mohajerin Esfahani, T. Keviczky. Stochastic Nonlinear Model Predictive Control of an Uncertain Batch Polymerisation Reactor. IFAC Conference on Nonlinear Model Predictive Control, Pp. 540-545. Seville, Spain. September, 2015\nV. Rostampour, K. Margellos, M. Vrakopoulou, M. Prandini, G. Andersson, J. Lygeros. Reserve Requirements in AC Power Systems With Uncertain Generation. IEEE Innovative Smart Grid Technologies Europe (ISGT), Pp. 1-5. Copenhagen, Denmark. October, 2013\n\n\n\n2013\n\nK. Margellos, V. Rostampour, M. Vrakopoulou, M. Prandini, G. Andersson, J. Lygeros. Stochastic Unit Commitment and Reserve Scheduling: A Tractable Formulation with Probabilistic Certificates. IEEE European Control Conference (ECC), Pp. 2513-2518. Zürich, Switzerland. June, 2013"
  },
  {
    "objectID": "publications.html#books-chapters",
    "href": "publications.html#books-chapters",
    "title": "Publications",
    "section": "Books & Chapters",
    "text": "Books & Chapters\n\n2019\n\nRostampour, Vahab, Ter Haar, Ole, Keviczky, Tamas. Computationally tractable reserve scheduling for AC power systems with wind power generation. Editors: Palensky P., Cvetković M., Keviczky T. (eds). Springer, Cham, Pp. 215-233, 2019.\nRostampour, Vahab, Ananduta, Wicak, Keviczky, Tamas. Distributed stochastic thermal energy management in smart thermal grids. Editors: Palensky P., Cvetković M., Keviczky T. (eds). Springer, Cham, Pp. 141-164, 2019."
  },
  {
    "objectID": "publications.html#dissertations",
    "href": "publications.html#dissertations",
    "title": "Publications",
    "section": "Dissertations",
    "text": "Dissertations\n\n2018 — September\n\nPh.D. Degree: Applied Mathematics and Control Systems Engineering\n\nThesis Title: Distributed Data-Driven Decision Making in Uncertain Networked Systems with Applications in Smart Energy Systems\nSupervisors:\n\nProf. Dr. Nathan van de Wouw (Eindhoven University of Technology)\nProf. Dr. Tamás Keviczky (Delft University of Technology)\n\nPhD Exam Committee:\n\nProf. Dr. Bart De Schutter (Delft University of Technology)\nProf. Dr. Peter Palensky (Delft University of Technology)\nProf. Dr. Riccardo Scattolini (Politecnico diMilano, Italy)\nProf. Dr. André Teixeira (Uppsala University, Sweden)\n\n\n\n\n\n2013 — April\n\nMS.c. Degree: Control Systems Engineering\n\nThesis Title: Tractable Reserve Scheduling Formulations for Power Systems with Uncertain Generation\nSupervisors:\n\nProf. Dr. John Lygeros (ETH Zurich)\nProf. Dr. Maria Prandini (Politecnico di Milano)\n\n\n\n\n\n2008 — April\n\nBS.c. Degree: Electrical and Electronic Engineering\n\nThesis Title: Web-Based Control Technologies for Smart Building Automations\nSupervisor:\n\nProf. Dr. Mahmoud Seifouri"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Blog",
    "section": "",
    "text": "Short updates, notes, and news.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnchored Kalman Filtering – Reproducible Environment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnchored Kalman Filtering in a Markov PD Model — Reproducible Demo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBook Reading Summary – Beyond Entrepreneurship 2.0\n\n\n\n\n\n\n\n\nSep 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBook Reading Summary – Zero to One\n\n\n\n\n\n\n\n\nSep 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPublication page is ready!\n\n\n\n\n\n\n\n\nSep 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHello, world\n\n\n\n\n\n\n\n\nSep 1, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/anchored-kf-env.html",
    "href": "posts/anchored-kf-env.html",
    "title": "Anchored Kalman Filtering – Reproducible Environment",
    "section": "",
    "text": "This page sets up a deterministic Python environment for experiments around anchored Kalman filtering in a Markov PD model, using only NumPy, Pandas, and Matplotlib. Each code block is explained in markdown before the code itself."
  },
  {
    "objectID": "posts/anchored-kf-env.html#overview",
    "href": "posts/anchored-kf-env.html#overview",
    "title": "Anchored Kalman Filtering – Reproducible Environment",
    "section": "",
    "text": "This page sets up a deterministic Python environment for experiments around anchored Kalman filtering in a Markov PD model, using only NumPy, Pandas, and Matplotlib. Each code block is explained in markdown before the code itself."
  },
  {
    "objectID": "posts/anchored-kf-env.html#imports-and-basic-configuration",
    "href": "posts/anchored-kf-env.html#imports-and-basic-configuration",
    "title": "Anchored Kalman Filtering – Reproducible Environment",
    "section": "Imports and Basic Configuration",
    "text": "Imports and Basic Configuration\nWe import the required libraries and use pathlib.Path for file handling.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom typing import Tuple, Dict"
  },
  {
    "objectID": "posts/anchored-kf-env.html#output-directory",
    "href": "posts/anchored-kf-env.html#output-directory",
    "title": "Anchored Kalman Filtering – Reproducible Environment",
    "section": "Output Directory",
    "text": "Output Directory\nFor portability, we create a local folder outputs/ that works in Quarto or Thebe.\n\nOUTDIR = Path(\"outputs\")\nOUTDIR.mkdir(parents=True, exist_ok=True)\nOUTDIR.resolve()\n\nWindowsPath('C:/Users/user/projects/vahabr.github.io/posts/outputs')"
  },
  {
    "objectID": "posts/anchored-kf-env.html#ratings-universe-and-defaults",
    "href": "posts/anchored-kf-env.html#ratings-universe-and-defaults",
    "title": "Anchored Kalman Filtering – Reproducible Environment",
    "section": "Ratings Universe and Defaults",
    "text": "Ratings Universe and Defaults\nWe define the rating states, their index mapping, and some defaults:\n\nT_DEFAULT: time horizon (quarters)\nN_DEFAULT: number of obligors\n\n\nRATINGS = [\"A\", \"B\", \"C\", \"D\"]\nIDX = {r: i for i, r in enumerate(RATINGS)}\nT_DEFAULT = 20    # quarters\nN_DEFAULT = 10_000\nRATINGS, IDX, T_DEFAULT, N_DEFAULT\n\n(['A', 'B', 'C', 'D'], {'A': 0, 'B': 1, 'C': 2, 'D': 3}, 20, 10000)"
  },
  {
    "objectID": "posts/anchored-kf-env.html#determinism-helper",
    "href": "posts/anchored-kf-env.html#determinism-helper",
    "title": "Anchored Kalman Filtering – Reproducible Environment",
    "section": "1) Determinism Helper",
    "text": "1) Determinism Helper\nTo make runs reproducible, we fix the random seed and return a local RNG (numpy.random.Generator).\n\ndef set_seed(seed: int) -&gt; np.random.Generator:\n    \"\"\"Set global determinism and return a local RNG.\"\"\"\n    np.random.seed(seed)\n    return np.random.default_rng(seed)\n\n# Example usage\nrng = set_seed(42)\nrng\n\nGenerator(PCG64) at 0x19836D55740"
  },
  {
    "objectID": "posts/anchored-kf-env.html#portfolio-initialisation",
    "href": "posts/anchored-kf-env.html#portfolio-initialisation",
    "title": "Anchored Kalman Filtering – Reproducible Environment",
    "section": "2) Portfolio Initialisation",
    "text": "2) Portfolio Initialisation\nThis function samples initial ratings for N obligors based on an input distribution pi0.\n\ndef gen_initial_portfolio(N: int, pi0: np.ndarray, rng: np.random.Generator=None) -&gt; np.ndarray:\n    \"\"\"Sample initial ratings for N obligors from distribution pi0.\"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n    labels = rng.choice(len(RATINGS), size=N, p=pi0)\n    return labels\n\nDemo: initialise a portfolio with a skew toward A and B.\n\npi0 = np.array([0.50, 0.35, 0.10, 0.05], dtype=float)\nlabels0 = gen_initial_portfolio(N_DEFAULT, pi0, rng)\n\n(pd.Series(labels0)\n   .map({i:r for r,i in IDX.items()})\n   .value_counts(normalize=True)\n   .rename(\"share\")\n   .to_frame()\n   .sort_index())\n\n\n\n\n\n\n\n\nshare\n\n\n\n\nA\n0.5015\n\n\nB\n0.3510\n\n\nC\n0.0974\n\n\nD\n0.0501"
  },
  {
    "objectID": "posts/anchored-kf-env.html#through-the-cycle-ttc-transition-matrix",
    "href": "posts/anchored-kf-env.html#through-the-cycle-ttc-transition-matrix",
    "title": "Anchored Kalman Filtering – Reproducible Environment",
    "section": "3) Through-the-Cycle (TTC) Transition Matrix",
    "text": "3) Through-the-Cycle (TTC) Transition Matrix\nWe construct the quarterly TTC matrix for states A, B, C, D with D absorbing.\n\ndef build_P_TTC() -&gt; np.ndarray:\n    \"\"\"Return the quarterly TTC transition matrix.\"\"\"\n    P_TTC = np.array([\n        [0.975, 0.022, 0.002, 0.001],\n        [0.030, 0.935, 0.030, 0.005],\n        [0.010, 0.060, 0.915, 0.015],\n        [0.000, 0.000, 0.000, 1.000],\n    ], dtype=float)\n    return P_TTC\n\nP_TTC = build_P_TTC()\nP_TTC\n\narray([[0.975, 0.022, 0.002, 0.001],\n       [0.03 , 0.935, 0.03 , 0.005],\n       [0.01 , 0.06 , 0.915, 0.015],\n       [0.   , 0.   , 0.   , 1.   ]])\n\n\nVerify that rows sum to 1 and visualise the transition matrix.\n\nrow_sums = P_TTC.sum(axis=1)\nassert np.allclose(row_sums, 1.0), \"Each row must sum to 1.\"\n\nfig, ax = plt.subplots(figsize=(4, 3))\nim = ax.imshow(P_TTC, aspect=\"auto\")\nax.set_xticks(range(len(RATINGS))); ax.set_xticklabels(RATINGS)\nax.set_yticks(range(len(RATINGS))); ax.set_yticklabels(RATINGS)\nax.set_title(\"Quarterly TTC Transition Matrix\")\nfig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nMinimal Sanity Run\nA helper to advance the portfolio one quarter using the TTC matrix. This confirms our setup is consistent.\n\ndef step_markov(labels: np.ndarray, P: np.ndarray, rng: np.random.Generator) -&gt; np.ndarray:\n    \"\"\"Advance one step in a Markov chain with transition matrix P.\"\"\"\n    n_states = P.shape[0]\n    u = rng.random(size=labels.shape[0])\n    next_labels = np.empty_like(labels)\n    cdf = P.cumsum(axis=1)\n    for s in range(n_states):\n        mask = (labels == s)\n        if not np.any(mask):\n            continue\n        next_labels[mask] = np.searchsorted(cdf[s], u[mask], side=\"right\")\n    return next_labels\n\nlabels1 = step_markov(labels0, P_TTC, rng)\n\npd.crosstab(\n    pd.Series(labels0).map({i:r for r,i in IDX.items()}),\n    pd.Series(labels1).map({i:r for r,i in IDX.items()}),\n    normalize=\"index\"\n).round(3)\n\n\n\n\n\n\n\ncol_0\nA\nB\nC\nD\n\n\nrow_0\n\n\n\n\n\n\n\n\nA\n0.975\n0.020\n0.003\n0.002\n\n\nB\n0.034\n0.936\n0.025\n0.005\n\n\nC\n0.011\n0.060\n0.909\n0.021\n\n\nD\n0.000\n0.000\n0.000\n1.000\n\n\n\n\n\n\n\n\nPerfect — that’s the Macroeconomic Scenarios block. To integrate it into your Quarto page the same way we did with Sections 1–3, we just add markdown explanations before each function group and keep the code in fenced {python} blocks.\nHere’s the .qmd-ready section you can paste directly under the earlier parts:"
  },
  {
    "objectID": "posts/anchored-kf-env.html#macroeconomic-scenarios",
    "href": "posts/anchored-kf-env.html#macroeconomic-scenarios",
    "title": "Anchored Kalman Filtering – Reproducible Environment",
    "section": "4) Macroeconomic Scenarios",
    "text": "4) Macroeconomic Scenarios\nWe create three stylised macroeconomic scenarios:\n\nBaseline: mild cycle with small fluctuations in GDP and unemployment.\nStress: sharp downturn followed by a slow recovery.\nPandemic: abrupt fall, fast rebound, with a short spike in unemployment.\n\nEach scenario returns quarterly paths for GDP growth and unemployment.\n\ndef _baseline_paths(T: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Baseline: mild cycle. GDP in pp q/q, Unemp level in pp.\"\"\"\n    t = np.arange(T)\n    gdp = 0.5 + 0.3*np.sin(2*np.pi*t/12)  # around 0.5% q/q, small cycle\n    unemp = 5.0 + 0.2*np.cos(2*np.pi*(t+3)/10)  # around 5%, small wiggle\n    return gdp, unemp\n\ndef _stress_paths(T: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Stress: sharp downturn, slow recovery.\"\"\"\n    t = np.arange(T)\n    gdp = 0.6 - 0.8*np.exp(-t/2.0)  # drop quickly then recover slowly\n    gdp[:4] -= 0.8  # accentuate early downturn\n    unemp = 4.5 + 1.8*(1 - np.exp(-t/4.0))  # rises then plateaus\n    return gdp, unemp\n\ndef _pandemic_paths(T: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Pandemic: abrupt fall, fast rebound (impose rebound at t=3 later).\"\"\"\n    t = np.arange(T)\n    gdp = 0.5*np.ones(T)\n    gdp[0:2] = -2.0  # abrupt fall\n    gdp[2] = -0.5\n    # Rebound will be imposed at t=3 in realised series\n    unemp = 4.2*np.ones(T)\n    unemp[0:3] = 5.5  # spike\n    unemp[3:6] = 5.0\n    return gdp, unemp\n\n\n\nForecasts vs Realisations\nWe simulate both forecasts (scenario paths) and realisations (forecast + Gaussian noise). For the pandemic scenario, we impose a strong rebound at quarter 3. We also construct a macro index from GDP and unemployment.\n\ndef gen_macro_forecasts_and_realised(\n    scenario: str, T: int = T_DEFAULT, rng: np.random.Generator = None\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Return dict with forecast and realised GDP/UNEMP and the macro index paths.\"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n    scenario = scenario.lower()\n    if scenario == \"baseline\":\n        gdp_f, unemp_f = _baseline_paths(T)\n    elif scenario == \"stress\":\n        gdp_f, unemp_f = _stress_paths(T)\n    elif scenario in {\"pandemic\", \"pandemic_shock\"}:\n        gdp_f, unemp_f = _pandemic_paths(T)\n    else:\n        raise ValueError(\"Unknown scenario\")\n\n    # Realised paths = forecast + Gaussian noise (σ ≈ 0.2pp)\n    gdp_r = gdp_f + rng.normal(0.0, 0.2, size=T)\n    unemp_r = unemp_f + rng.normal(0.0, 0.2, size=T)\n\n    # Pandemic rebound at t=3\n    if scenario in {\"pandemic\", \"pandemic_shock\"} and T &gt;= 4:\n        gdp_r[3] += 2.5\n        unemp_r[3] -= 0.8\n\n    # Macro indices\n    M_hat = macro_index_from_gdp_unemp(gdp_f, unemp_f)\n    M_real = macro_index_from_gdp_unemp(gdp_r, unemp_r)\n\n    return {\n        \"gdp_forecast\": gdp_f, \"unemp_forecast\": unemp_f,\n        \"gdp_realised\": gdp_r, \"unemp_realised\": unemp_r,\n        \"M_hat\": M_hat, \"M_real\": M_real\n    }\n\n\n\n\nMacro Index Construction\nWe define a simple index combining standardised GDP and unemployment:\n\\[\nM_t = 0.5 \\, z(\\text{GDP}_t) \\;-\\; 0.5 \\, z(\\text{UNEMP}_t)\n\\]\nwhere \\(z(\\cdot)\\) is a z-score normalisation.\n\ndef macro_index_from_gdp_unemp(GDP: np.ndarray, UNEMP: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Return M_t = 0.5*z(GDP) - 0.5*z(UNEMP).\"\"\"\n    def z(x: np.ndarray) -&gt; np.ndarray:\n        mu = np.mean(x)\n        sigma = np.std(x, ddof=0)\n        sigma = sigma if sigma &gt; 1e-12 else 1.0\n        return (x - mu) / sigma\n    return 0.5*z(GDP) - 0.5*z(UNEMP)\n\n\n\n\nQuick Demo\nGenerate and plot the three scenarios to see the difference.\n\nscenarios = [\"baseline\", \"stress\", \"pandemic\"]\n\nfig, axes = plt.subplots(3, 2, figsize=(10, 8), sharex=True)\nfor i, sc in enumerate(scenarios):\n    res = gen_macro_forecasts_and_realised(sc, T=20, rng=rng)\n    axes[i,0].plot(res[\"gdp_forecast\"], label=\"Forecast\")\n    axes[i,0].plot(res[\"gdp_realised\"], label=\"Realised\", alpha=0.7)\n    axes[i,0].set_title(f\"{sc.title()} GDP\")\n    axes[i,0].legend()\n    axes[i,1].plot(res[\"unemp_forecast\"], label=\"Forecast\")\n    axes[i,1].plot(res[\"unemp_realised\"], label=\"Realised\", alpha=0.7)\n    axes[i,1].set_title(f\"{sc.title()} Unemployment\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/anchored-kf-env.html#pit-overlay-point-in-time-transitions",
    "href": "posts/anchored-kf-env.html#pit-overlay-point-in-time-transitions",
    "title": "Anchored Kalman Filtering – Reproducible Environment",
    "section": "5) PIT Overlay (Point-in-Time Transitions)",
    "text": "5) PIT Overlay (Point-in-Time Transitions)\nWe tilt the TTC matrix with a logit-style overlay driven by the macro index \\(M_t\\). The tilt strengths are in a \\(\\beta\\) matrix: positive entries push downgrades/defaults up in bad times (negative \\(M_t\\)), and upgrades are symmetric negatives. After tilting, we row-normalise and keep default (D) absorbing.\n\ndef _build_betas() -&gt; np.ndarray:\n    \"\"\"Beta matrix β_ij with specified nearest-neighbour/default structure.\"\"\"\n    beta = np.zeros((4,4), dtype=float)\n\n    # Downgrades / defaults (given)\n    beta[IDX[\"A\"], IDX[\"B\"]] = 2.0\n    beta[IDX[\"A\"], IDX[\"C\"]] = 2.5\n    beta[IDX[\"A\"], IDX[\"D\"]] = 3.0\n\n    beta[IDX[\"B\"], IDX[\"C\"]] = 1.5\n    beta[IDX[\"B\"], IDX[\"D\"]] = 2.0\n\n    beta[IDX[\"C\"], IDX[\"D\"]] = 1.2\n\n    # Upgrades are negatives\n    beta[IDX[\"B\"], IDX[\"A\"]] = -beta[IDX[\"A\"], IDX[\"B\"]]\n    beta[IDX[\"C\"], IDX[\"B\"]] = -beta[IDX[\"B\"], IDX[\"C\"]]\n    beta[IDX[\"C\"], IDX[\"A\"]] = -beta[IDX[\"A\"], IDX[\"C\"]]\n    # Diagonals and other moves left at 0.0\n    return beta\n\n\ndef pit_overlay(P_TTC: np.ndarray, M_t: float, betas: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Return a PIT transition matrix for the given macro index M_t.\"\"\"\n    P = np.array(P_TTC, dtype=float)\n    # Logit-style tilt (elementwise)\n    W = P * np.exp(betas * M_t)\n\n    # Row normalisation; keep D absorbing\n    for i in range(W.shape[0]):\n        if i == IDX[\"D\"]:\n            W[i, :] = 0.0\n            W[i, IDX[\"D\"]] = 1.0\n        else:\n            s = W[i, :].sum()\n            if s &lt;= 0 or not np.isfinite(s):\n                # Fallback to TTC row if degenerate\n                W[i, :] = P[i, :]\n            else:\n                W[i, :] = W[i, :] / s\n    return W\n\nNotes\n\n\\(M_t\\) is in z-units from your macro index; typical magnitudes are small, so exp(betas*M_t) remains well-behaved.\nWe defensively fall back to TTC if a row becomes degenerate."
  },
  {
    "objectID": "posts/anchored-kf-env.html#kalman-filters-naïve-vs-anchored",
    "href": "posts/anchored-kf-env.html#kalman-filters-naïve-vs-anchored",
    "title": "Anchored Kalman Filtering – Reproducible Environment",
    "section": "6) Kalman Filters (Naïve vs Anchored)",
    "text": "6) Kalman Filters (Naïve vs Anchored)\nWe compare a naïve KF (observations are the forecast index \\(M^{\\hat{}}_t\\)) with an anchored KF that adds a second observation (“anchor”) that keeps the level close to a prior (0) with a tunable variance. Before the anchoring date \\(T_{\\text{anchor}}\\), the anchor is soft (variance \\(\\sigma_\\*^2\\)); after it, the anchor becomes hard (very small variance).\n\ndef kalman_naive(M_hat: np.ndarray, rho: float = 0.90, Q: float = None, R: float = 0.25) -&gt; np.ndarray:\n    \"\"\"Naïve KF: state m_t is AR(1); observation y_t = M_hat[t].\"\"\"\n    T = len(M_hat)\n    if Q is None:\n        Q = 1 - rho**2  # keeps stationary variance ≈1\n\n    # State init\n    m = 0.0\n    P = 1.0\n    H = 1.0\n\n    m_filt = np.zeros(T)\n    for t in range(T):\n        # Predict\n        m_pred = rho * m\n        P_pred = rho * P * rho + Q\n\n        # Update with y_t = M_hat[t]\n        y = M_hat[t]\n        S = H * P_pred * H + R          # scalar\n        K = P_pred * H / S               # scalar\n        m = m_pred + K * (y - H * m_pred)\n        P = (1 - K * H) * P_pred\n\n        m_filt[t] = m\n    return m_filt\n\n\ndef kalman_anchored(\n    M_hat: np.ndarray,\n    T_anchor: int = 20,\n    rho: float = 0.90,\n    Q: float = None,\n    R: float = 0.25,\n    sigma_star2_pre: float = 0.25\n) -&gt; np.ndarray:\n    \"\"\"\n    Anchored KF with stacked observation y_t = [M_hat_t, 0]^T, H = [[1],[1]].\n    - Before T_anchor: soft anchor with variance sigma_star2_pre.\n    - On/after T_anchor: hard anchor (very small variance).\n    \"\"\"\n    T = len(M_hat)\n    if Q is None:\n        Q = 1 - rho**2\n\n    m = 0.0\n    P = 1.0\n\n    H = np.array([[1.0],[1.0]])  # (2,1)\n    m_filt = np.zeros(T)\n\n    for t in range(T):\n        # Predict\n        m_pred = rho * m\n        P_pred = rho * P * rho + Q\n\n        # Stacked obs\n        y = np.array([M_hat[t], 0.0])        # (2,)\n        if t &lt; T_anchor:\n            R_aug = np.diag([R, sigma_star2_pre])\n        else:\n            R_aug = np.diag([R, 1e-12])      # hard anchor\n\n        # Innovation covariance: S = H P_pred H^T + R\n        # Use solves for stability (avoid explicit inverse)\n        S = H @ np.array([[P_pred]]) @ H.T + R_aug        # (2,2)\n        # K = P_pred H^T S^{-1}\n        K = (np.array([[P_pred]]) @ H.T) @ np.linalg.solve(S, np.eye(2))  # (1,2)\n\n        # Innovation: y - H m_pred\n        innov = y - (H[:, 0] * m_pred)       # (2,)\n        m = m_pred + (K @ innov)[0]          # scalar\n        P = (1.0 - (K @ H)[0, 0]) * P_pred   # scalar\n\n        m_filt[t] = m\n    return m_filt\n\nTips\n\nIf you’re on Python 3.8, you don’t need generics (e.g. tuple[...]); these functions avoid them.\nIf you ever see type-subscript errors again, either switch to typing.Tuple[...] or pin Python 3.11+ with a runtime.txt (python-3.11) for Binder/Thebe.\n\n\n\nQuick demo: compare naïve vs anchored KF\nThis cell assumes you’ve already generated a macro scenario and have its forecast index M_hat (e.g., from Section 4). It plots both filtered state paths side by side.\n\n# Example: use a baseline scenario's forecast macro index\nbetas = _build_betas()  # not used here, but ensures PIT section is executed\n\ndemo = gen_macro_forecasts_and_realised(\"baseline\", T=40, rng=rng)\nMhat_demo = demo[\"M_hat\"]\n\nm_naive = kalman_naive(Mhat_demo, rho=0.9, R=0.25)\nm_anchor = kalman_anchored(Mhat_demo, T_anchor=20, rho=0.9, R=0.25, sigma_star2_pre=0.25)\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.plot(Mhat_demo, label=\"Forecast index  $M^\\\\hat{}_t$\", alpha=0.6)\nax.plot(m_naive, label=\"Naïve KF  $m_t$\")\nax.plot(m_anchor, label=\"Anchored KF  $m_t$\")\nax.axvline(20, ls=\"--\", alpha=0.5, label=\"Anchor switch (T_anchor)\")\nax.set_title(\"Naïve vs Anchored Kalman Filter (Baseline scenario)\")\nax.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/anchored-kf-env.html#propagation",
    "href": "posts/anchored-kf-env.html#propagation",
    "title": "Anchored Kalman Filtering – Reproducible Environment",
    "section": "7) Propagation",
    "text": "7) Propagation\nWe propagate an initial cross-sectional distribution \\(\\pi_0\\) through a time-varying sequence of transition matrices \\(\\{P_t\\}_{t=1}^T\\). We also extract the default mass series \\(Y_t = \\pi_t[D]\\).\n\nfrom typing import List, Dict, Tuple  # 3.8-safe generics\n\ndef propagate_distribution(pi0: np.ndarray, P_ts: List[np.ndarray]) -&gt; np.ndarray:\n    \"\"\"Return array of shape (T+1, 4) of distributions over time.\"\"\"\n    T = len(P_ts)\n    pi = np.zeros((T+1, len(RATINGS)))\n    pi[0, :] = pi0\n    current = pi0.copy()\n    for t in range(T):\n        current = current @ P_ts[t]\n        pi[t+1, :] = current\n    return pi\n\ndef compute_pd_series(pi_ts: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Return Y_t = probability mass in default at each t.\"\"\"\n    return pi_ts[:, IDX[\"D\"]]"
  },
  {
    "objectID": "posts/anchored-kf-env.html#experiment-runner",
    "href": "posts/anchored-kf-env.html#experiment-runner",
    "title": "Anchored Kalman Filtering – Reproducible Environment",
    "section": "8) Experiment Runner",
    "text": "8) Experiment Runner\nThis wraps everything: scenario → macro paths → filtering method → PIT matrices → PD term structure. It also saves CSVs and a figure to your local outputs/ directory.\n\ndef run_experiment(\n    scenario: str,\n    method: str,\n    N: int = N_DEFAULT,\n    T: int = T_DEFAULT,\n    seed: int = 12345\n) -&gt; Dict[str, object]:\n    \"\"\"Run scenario × method. Save CSVs and figures. Return key arrays + file paths.\"\"\"\n    rng = set_seed(seed + hash((scenario, method)) % 10_000)  # slight offset per run\n\n    # Inputs\n    pi0 = np.array([0.45, 0.40, 0.15, 0.00], dtype=float)\n    labels0 = gen_initial_portfolio(N, pi0, rng=rng)\n    P_TTC = build_P_TTC()\n    betas = _build_betas()\n\n    # Macro\n    macro = gen_macro_forecasts_and_realised(scenario, T=T, rng=rng)\n    M_hat = macro[\"M_hat\"]\n    M_real = macro[\"M_real\"]\n\n    # Filtering method selection\n    if method == \"raw\":\n        M_est = M_real.copy()\n        M_eff = M_est.copy()\n    elif method == \"naive\":\n        M_est = kalman_naive(M_hat, rho=0.90, Q=None, R=0.25)\n        M_eff = M_est.copy()\n    elif method == \"anchored\":\n        M_est = kalman_anchored(M_hat, T_anchor=T, rho=0.90, Q=None, R=0.25, sigma_star2_pre=0.25)\n        M_eff = M_est.copy()\n    else:\n        raise ValueError(\"Unknown method\")\n\n    # Time-varying PIT matrices\n    P_ts = [pit_overlay(P_TTC, float(M_eff[t]), betas) for t in range(T)]\n\n    # Propagate\n    pi_ts = propagate_distribution(pi0, P_ts)\n    Y_t = compute_pd_series(pi_ts)\n\n    # TTC baseline path (neutral M=0)\n    P_neutral = [P_TTC.copy() for _ in range(T)]\n    pi_ttc = propagate_distribution(pi0, P_neutral)\n    Y_ttc = compute_pd_series(pi_ttc)\n\n    # Save transition matrices\n    tm_rows = []\n    for t in range(T):\n        for i, ri in enumerate(RATINGS):\n            for j, rj in enumerate(RATINGS):\n                tm_rows.append({\"t\": t+1, \"from\": ri, \"to\": rj, \"P\": P_ts[t][i, j]})\n    df_tm = pd.DataFrame(tm_rows)\n    f_tm = OUTDIR / f\"transition_matrices_{scenario}_{method}.csv\"\n    df_tm.to_csv(f_tm, index=False)\n\n    # Save macro paths\n    df_macro = pd.DataFrame({\n        \"t\": np.arange(1, T+1),\n        \"M_forecast\": M_hat,\n        \"M_realised\": M_real,\n        \"M_estimate\": M_est\n    })\n    f_macro = OUTDIR / f\"macro_paths_{scenario}_{method}.csv\"\n    df_macro.to_csv(f_macro, index=False)\n\n    # Save PD term structures\n    df_pd = pd.DataFrame({\n        \"t\": np.arange(0, T+1),\n        \"Y_t\": Y_t,\n        \"Y_ttc\": Y_ttc\n    })\n    f_pd = OUTDIR / f\"pd_term_structures_{scenario}_{method}.csv\"\n    df_pd.to_csv(f_pd, index=False)\n\n    # Figure: macro filter\n    fig1 = plt.figure()\n    plt.plot(np.arange(1, T+1), M_hat, label=\"Forecast (M̂)\")\n    plt.plot(np.arange(1, T+1), M_real, label=\"Realised M\")\n    plt.plot(np.arange(1, T+1), M_est, label=f\"Estimate: {method}\")\n    plt.axhline(0.0)\n    plt.xlabel(\"Quarter\")\n    plt.ylabel(\"Macro index (z)\")\n    plt.title(f\"Macro filter: {scenario} × {method}\")\n    plt.legend()\n    f_fig1 = OUTDIR / f\"macro_filter_{scenario}_{method}.png\"\n    fig1.savefig(f_fig1, bbox_inches=\"tight\")\n    plt.close(fig1)\n\n    return {\n        \"labels0\": labels0,\n        \"P_ts\": P_ts,\n        \"pi_ts\": pi_ts,\n        \"Y_t\": Y_t,\n        \"Y_ttc\": Y_ttc,\n        \"M_hat\": M_hat,\n        \"M_real\": M_real,\n        \"M_est\": M_est,\n        \"files\": {\n            \"transition_matrices\": str(f_tm),\n            \"macro_paths\": str(f_macro),\n            \"pd_term_structures\": str(f_pd),\n            \"macro_figure\": str(f_fig1),\n        },\n    }"
  },
  {
    "objectID": "posts/anchored-kf-env.html#pd-bands-and-metrics",
    "href": "posts/anchored-kf-env.html#pd-bands-and-metrics",
    "title": "Anchored Kalman Filtering – Reproducible Environment",
    "section": "9) PD Bands and Metrics",
    "text": "9) PD Bands and Metrics\nWe plot PD term-structure bands across scenarios for a fixed method, compute cross-scenario variance of \\(Y_t\\), and estimate loss volatility via simple macro-noise Monte Carlo.\n\ndef _pd_bands_figure(results_per_scn: Dict[str, Dict[str, object]], method: str, T: int = T_DEFAULT) -&gt; str:\n    \"\"\"Create PD bands plot per method across scenarios. Return file path.\"\"\"\n    tgrid = np.arange(0, T+1)\n    fig = plt.figure()\n    # TTC baseline from first scenario's output\n    any_key = next(iter(results_per_scn))\n    Y_ttc = results_per_scn[any_key][\"Y_ttc\"]\n    plt.plot(tgrid, Y_ttc, label=\"TTC baseline\")\n    # Scenario paths\n    for scn, res in results_per_scn.items():\n        plt.plot(tgrid, res[\"Y_t\"], label=f\"{scn}\")\n    plt.xlabel(\"Quarter\")\n    plt.ylabel(\"Cumulative PD (π_t[D])\")\n    plt.title(f\"PD term-structure bands across scenarios — {method}\")\n    plt.legend()\n    fpath = OUTDIR / f\"pd_bands_{method}.png\"\n    fig.savefig(fpath, bbox_inches=\"tight\")\n    plt.close(fig)\n    return str(fpath)\n\ndef variance_across_scenarios(Y_by_scn: Dict[str, np.ndarray]) -&gt; pd.DataFrame:\n    \"\"\"Variance of Y_t across scenarios for each horizon.\"\"\"\n    # Y_by_scn: {scenario: Y_t array}\n    Ys = np.stack([v for v in Y_by_scn.values()], axis=0)  # (n_scn, T+1)\n    var_t = Ys.var(axis=0, ddof=0)\n    df = pd.DataFrame({\n        \"t\": np.arange(Ys.shape[1]),\n        \"var_Y_t\": var_t\n    })\n    return df\n\ndef monte_carlo_loss_volatility(\n    scenario: str,\n    method: str,\n    pi0: np.ndarray,\n    P_TTC: np.ndarray,\n    betas: np.ndarray,\n    base_forecasts: Dict[str, np.ndarray],\n    n_rep: int = 200,\n    seed: int = 777\n) -&gt; Dict[str, float]:\n    \"\"\"Std of final loss fraction Y_T across forecast-noise replications.\"\"\"\n    rng = np.random.default_rng(seed + hash((scenario, method)) % 10_000)\n    T = len(base_forecasts[\"M_hat\"])\n    YT = np.zeros(n_rep)\n    for r in range(n_rep):\n        # Realised from forecast + noise (+ pandemic rebound if relevant)\n        gdp_f = base_forecasts[\"gdp_forecast\"]\n        un_f = base_forecasts[\"unemp_forecast\"]\n        gdp_r = gdp_f + rng.normal(0.0, 0.2, size=T)\n        un_r = un_f + rng.normal(0.0, 0.2, size=T)\n        if scenario.lower().startswith(\"pandemic\") and T &gt;= 4:\n            gdp_r[3] += 2.5\n            un_r[3] -= 0.8\n        M_hat = macro_index_from_gdp_unemp(gdp_f, un_f)\n        M_real = macro_index_from_gdp_unemp(gdp_r, un_r)\n\n        # Method\n        if method == \"raw\":\n            M_eff = M_real\n        elif method == \"naive\":\n            M_eff = kalman_naive(M_hat, rho=0.90, Q=None, R=0.25)\n        elif method == \"anchored\":\n            M_eff = kalman_anchored(M_hat, T_anchor=T, rho=0.90, Q=None, R=0.25, sigma_star2_pre=0.25)\n        else:\n            raise ValueError(\"Unknown method\")\n\n        P_ts = [pit_overlay(P_TTC, float(M_eff[t]), betas) for t in range(T)]\n        pi_ts = propagate_distribution(pi0, P_ts)\n        YT[r] = pi_ts[-1, IDX[\"D\"]]\n    return {\"mean\": float(YT.mean()), \"std\": float(YT.std(ddof=0))}\n\n\n\nQuick end-to-end demo\nThis runs all three scenarios with the anchored method, creates the PD bands chart, prints variance across scenarios, and shows where files were saved.\n\nscenarios = [\"baseline\", \"stress\", \"pandemic\"]\nmethod = \"anchored\"\nT_demo = 20\n\nresults = {sc: run_experiment(sc, method=method, T=T_demo, seed=2025) for sc in scenarios}\n\n# PD bands figure\nband_path = _pd_bands_figure(results, method=method, T=T_demo)\n\n# Variance across scenarios\nvar_df = variance_across_scenarios({sc: res[\"Y_t\"] for sc, res in results.items()})\ndisplay(var_df.head(8))\n\nprint(\"Saved files:\")\nfor sc, res in results.items():\n    print(f\"  [{sc}] macro: {res['files']['macro_paths']}\")\n    print(f\"  [{sc}] PDs  : {res['files']['pd_term_structures']}\")\nprint(f\"  Bands figure: {band_path}\")\n\n\n\n\n\n\n\n\nt\nvar_Y_t\n\n\n\n\n0\n0\n0.000000\n\n\n1\n1\n0.000004\n\n\n2\n2\n0.000025\n\n\n3\n3\n0.000088\n\n\n4\n4\n0.000202\n\n\n5\n5\n0.000317\n\n\n6\n6\n0.000388\n\n\n7\n7\n0.000392\n\n\n\n\n\n\n\nSaved files:\n  [baseline] macro: outputs\\macro_paths_baseline_anchored.csv\n  [baseline] PDs  : outputs\\pd_term_structures_baseline_anchored.csv\n  [stress] macro: outputs\\macro_paths_stress_anchored.csv\n  [stress] PDs  : outputs\\pd_term_structures_stress_anchored.csv\n  [pandemic] macro: outputs\\macro_paths_pandemic_anchored.csv\n  [pandemic] PDs  : outputs\\pd_term_structures_pandemic_anchored.csv\n  Bands figure: outputs\\pd_bands_anchored.png"
  },
  {
    "objectID": "posts/anchored-kf-env.html#main-reproducible-end-to-end-run",
    "href": "posts/anchored-kf-env.html#main-reproducible-end-to-end-run",
    "title": "Anchored Kalman Filtering – Reproducible Environment",
    "section": "10) Main (reproducible end-to-end run)",
    "text": "10) Main (reproducible end-to-end run)\nThis orchestrates everything: runs all scenarios × methods, saves outputs (CSVs + PNGs) under outputs/, creates a file registry, and prints a couple of table heads for a quick sanity check.\n\nimport zlib  # stable seed offsets\n\ndef _stable_offset(scenario: str, method: str, mod: int = 10_000) -&gt; int:\n    \"\"\"Deterministic offset for seeds (independent of Python hash randomization).\"\"\"\n    key = f\"{scenario}|{method}\".encode()\n    return zlib.crc32(key) % mod\n\ndef main():\n    seed = 20250821\n    rng = set_seed(seed)\n\n    scenarios = [\"baseline\", \"stress\", \"pandemic\"]\n    methods = [\"raw\", \"naive\", \"anchored\"]\n    pi0 = np.array([0.45, 0.40, 0.15, 0.00], dtype=float)\n    P_TTC = build_P_TTC()\n    betas = _build_betas()\n\n    # Run experiments\n    all_results = {m: {} for m in methods}\n    file_registry = []\n\n    for m in methods:\n        for scn in scenarios:\n            # Stable, per-run offset for determinism across environments\n            res = run_experiment(\n                scn, m,\n                N=N_DEFAULT,\n                T=T_DEFAULT,\n                seed=seed + _stable_offset(scn, m)\n            )\n            all_results[m][scn] = res\n            files = res[\"files\"]\n            file_registry.append({\n                \"scenario\": scn,\n                \"method\": m,\n                **files\n            })\n\n    # PD bands per method and variance across scenarios\n    var_tables = []\n    pd_band_files = []\n    for m in methods:\n        # Bands figure\n        f = _pd_bands_figure(all_results[m], m, T=T_DEFAULT)\n        pd_band_files.append({\"method\": m, \"pd_bands_figure\": f})\n\n        # Variance table\n        Y_by_scn = {scn: all_results[m][scn][\"Y_t\"] for scn in scenarios}\n        df_var = variance_across_scenarios(Y_by_scn)\n        df_var.insert(0, \"method\", m)\n        var_tables.append(df_var)\n\n    df_var_all = pd.concat(var_tables, ignore_index=True)\n    f_var = OUTDIR / \"variance_Y_across_scenarios.csv\"\n    df_var_all.to_csv(f_var, index=False)\n\n    # Monte Carlo lifetime loss volatility per scenario × method\n    mc_rows = []\n    for m in methods:\n        for scn in scenarios:\n            base_forecasts = gen_macro_forecasts_and_realised(scn, T=T_DEFAULT, rng=rng)\n            mc = monte_carlo_loss_volatility(\n                scn, m, pi0, P_TTC, betas, base_forecasts,\n                n_rep=200, seed=seed + _stable_offset(scn, m)\n            )\n            mc_rows.append({\"scenario\": scn, \"method\": m, \"YT_mean\": mc[\"mean\"], \"YT_std\": mc[\"std\"]})\n    df_mc = pd.DataFrame(mc_rows)\n    f_mc = OUTDIR / \"lifetime_loss_volatility_mc.csv\"\n    df_mc.to_csv(f_mc, index=False)\n\n    # Summary metrics file combining tables\n    df_summary = pd.DataFrame({\n        \"file\": [\n            str(f_var),\n            str(f_mc),\n        ],\n        \"description\": [\n            \"Variance of Y_t across scenarios by horizon and method\",\n            \"Lifetime loss volatility (std of Y_T) across 200 MC reps per scenario×method\",\n        ]\n    })\n    f_summary = OUTDIR / \"summary_metrics.csv\"\n    df_summary.to_csv(f_summary, index=False)\n\n    # Append band figures to registry and write a registry CSV\n    for item in pd_band_files:\n        file_registry.append({\n            \"scenario\": \"all\",\n            \"method\": item[\"method\"],\n            \"macro_paths\": \"\",\n            \"transition_matrices\": \"\",\n            \"pd_term_structures\": \"\",\n            \"macro_figure\": \"\",\n            \"pd_bands_figure\": item[\"pd_bands_figure\"]\n        })\n    df_registry = pd.DataFrame(file_registry)\n    f_registry = OUTDIR / \"file_registry.csv\"\n    df_registry.to_csv(f_registry, index=False)\n\n    # Return a small dict of paths for printing\n    return {\n        \"registry\": str(f_registry),\n        \"variance_table\": str(f_var),\n        \"mc_table\": str(f_mc),\n        \"summary\": str(f_summary),\n        \"example_macro\": all_results[\"naive\"][\"baseline\"][\"files\"][\"macro_paths\"],\n        \"example_pd\": all_results[\"anchored\"][\"pandemic\"][\"files\"][\"pd_term_structures\"],\n    }\n\n# Execute main and print heads\npaths = main()\n\nprint(\"Saved files (selected):\")\nfor k, v in paths.items():\n    print(f\" - {k}: {v}\")\n\nprint(\"\\nExample heads:\")\ndf_macro = pd.read_csv(paths[\"example_macro\"])\ndf_pd = pd.read_csv(paths[\"example_pd\"])\n\nprint(\"\\nmacro_paths (baseline × naive) head:\")\nprint(df_macro.head(6))\n\nprint(\"\\npd_term_structures (pandemic × anchored) head:\")\nprint(df_pd.head(6))\n\nSaved files (selected):\n - registry: outputs\\file_registry.csv\n - variance_table: outputs\\variance_Y_across_scenarios.csv\n - mc_table: outputs\\lifetime_loss_volatility_mc.csv\n - summary: outputs\\summary_metrics.csv\n - example_macro: outputs\\macro_paths_baseline_naive.csv\n - example_pd: outputs\\pd_term_structures_pandemic_anchored.csv\n\nExample heads:\n\nmacro_paths (baseline × naive) head:\n   t  M_forecast  M_realised  M_estimate\n0  1    0.096192   -0.177080    0.076954\n1  2    0.828193    0.932838    0.513020\n2  3    1.240280    0.989266    0.891716\n3  4    1.206640    0.858906    1.023572\n4  5    0.751682    1.044541    0.828636\n5  6    0.037623   -0.011020    0.359169\n\npd_term_structures (pandemic × anchored) head:\n   t       Y_t     Y_ttc\n0  0  0.000000  0.000000\n1  1  0.000663  0.004700\n2  2  0.001103  0.009369\n3  3  0.001729  0.014006\n4  4  0.003144  0.018610\n5  5  0.004981  0.023181\n\n\nWhy the crc32 change matters Using hash((scenario, method)) makes seeds differ across runs (Python salts hash() for security). zlib.crc32 on a string key is stable everywhere, so your seeds — and outputs — are truly reproducible."
  },
  {
    "objectID": "posts/2025-09-03-BE2.html",
    "href": "posts/2025-09-03-BE2.html",
    "title": "Book Reading Summary – Beyond Entrepreneurship 2.0",
    "section": "",
    "text": "It’s not a management book. It’s a leadership manifesto for building companies that outlast their founders by embedding purpose, discipline, and a culture of greatness. The updated 2.0 edition integrates lessons from Good to Great, Built to Last, Great by Choice, and others — giving it both emotional weight (due to Lazier’s legacy) and strategic punch. The main strengths of the book is as follows:\n\nTimeless principles on vision, values, and disciplined leadership.\nPowerful structure around Level 5 Leadership, Flywheel, Clock Building, and 20 Mile March — proven across real companies.\nEncourages long-term thinking and principled ambition over flashy growth."
  },
  {
    "objectID": "posts/2025-09-03-BE2.html#core-idea",
    "href": "posts/2025-09-03-BE2.html#core-idea",
    "title": "Book Reading Summary – Beyond Entrepreneurship 2.0",
    "section": "",
    "text": "It’s not a management book. It’s a leadership manifesto for building companies that outlast their founders by embedding purpose, discipline, and a culture of greatness. The updated 2.0 edition integrates lessons from Good to Great, Built to Last, Great by Choice, and others — giving it both emotional weight (due to Lazier’s legacy) and strategic punch. The main strengths of the book is as follows:\n\nTimeless principles on vision, values, and disciplined leadership.\nPowerful structure around Level 5 Leadership, Flywheel, Clock Building, and 20 Mile March — proven across real companies.\nEncourages long-term thinking and principled ambition over flashy growth."
  },
  {
    "objectID": "posts/2025-09-03-BE2.html#personal-frank-view",
    "href": "posts/2025-09-03-BE2.html#personal-frank-view",
    "title": "Book Reading Summary – Beyond Entrepreneurship 2.0",
    "section": "Personal Frank View",
    "text": "Personal Frank View\nIt is a valuable synthesis for founders and leaders serious about building enduring companies. However, if you’re looking for startup tactics, you should skip this book for now. It’s a slow-burn book with long-term payoff."
  },
  {
    "objectID": "posts/2025-09-03-BE2.html#chapters-summary",
    "href": "posts/2025-09-03-BE2.html#chapters-summary",
    "title": "Book Reading Summary – Beyond Entrepreneurship 2.0",
    "section": "Chapters Summary",
    "text": "Chapters Summary\n\nChapter 1: Bill and Me\n\nThe purpose of life is not to be happy. The purpose of life is to be useful.\n\nJim Collins opens the book with a heartfelt reflection on his friendship and intellectual partnership with Bill Lazier, co-author of the original Beyond Entrepreneurship. Far from being just a professional collaborator, Lazier was Collins’ mentor, a moral compass, and an enduring example of what it means to lead with character. The chapter is a personal tribute, but also a profound philosophical statement: at the core of great leadership lies not brilliance or ambition, but decency, trustworthiness, and humility.\nThrough anecdotes and quiet observations, Collins illustrates the depth of Bill’s influence. Lazier believed that the greatest currency in business — and in life — is trust. He practiced what he preached, consistently treating others with generosity and respect, even when it cost him personally. Collins credits Lazier with shaping many of the principles that would later become famous in Good to Great and Built to Last, particularly the emphasis on people, values, and long-term integrity over short-term gains.\nThe chapter establishes a central idea: great businesses are built by great people, and great people are shaped by their values, not their titles or credentials. Collins makes clear that this book — BE 2.0 — is not merely an update of a classic, but a deeply personal continuation of a shared legacy. “The purpose of life,” Lazier once told him, “is not to be happy. The purpose of life is to be useful.” That sentiment echoes through the entire book.\n\n\nChapter 2: Great Vision Without Great People Is Irrelevant\n\nIf you have the right people, the problem of how to motivate and manage largely goes away.\n\nIn this chapter, Collins delivers one of his core convictions: that the quality of your people is more important than the brilliance of your vision. A company with an extraordinary vision but the wrong people will fail. A company with average strategy but extraordinary people has a real chance at greatness. This concept, which he later famously summarised as “first who, then what,” emphasises that effective leaders should prioritise assembling the right team before finalising strategy, direction, or product plans.\nCollins urges leaders to ask themselves a simple but powerful question: “Would you hire this person again?” If the answer is no, it’s a sign to act — not necessarily by firing immediately, but by being honest about fit. The right people are not only competent, but also aligned in values, self-motivated, and capable of growing with the organisation. They don’t require constant supervision. Conversely, the wrong people, no matter how talented, create drag — they weaken the culture, confuse the mission, and drain leadership energy.\nImportantly, Collins warns against a common mistake: tolerating mediocrity out of loyalty. Just because someone has been with the company since the beginning doesn’t mean they should stay. Companies that scale with discipline make tough people decisions early. Letting a mismatched team member stay in a key seat can be more damaging than any strategic misstep.\nThe chapter also discusses the danger of keeping “clever but toxic” individuals. High performance is not enough if the behaviour erodes trust or undermines others. A strong culture is not built by a few top performers — it is built by consistency of values across the organisation. In short, leaders must view talent not as a transaction, but as a cultural foundation.\n\n\nChapter 3: Leadership Style\n\nYour leadership style is not what you say it is. It’s what you do consistently, even when no one is watching.\n\nIn this chapter, Collins examines the subtle but critical difference between managing people and leading a culture. The most effective leaders, he argues, don’t impose control — they create the conditions in which people naturally operate with discipline, creativity, and accountability. Leadership, in this sense, is not about style in a superficial or performative way. It’s about what you repeatedly do, what you tolerate, and what you reward — because over time, that becomes your culture.\nThe chapter dismantles the myth of the charismatic, top-down leader. While charisma can inspire short bursts of motivation, it doesn’t scale. What scales is consistency. Collins highlights the power of “clock-building,” a metaphor for creating a self-sustaining, values-driven organisation that runs well without you. Great leaders focus not on being indispensable, but on building systems, people, and processes that are.\nCollins introduces several dimensions of leadership that go beyond traditional “styles” like authoritarian vs participative. He emphasises Level 5 Leadership — a concept expanded in Good to Great — which blends personal humility with fierce resolve. These leaders aren’t loud, but they are unwavering. They’re not the centre of attention, but they set a tone that permeates the organisation. Importantly, they take responsibility for failure and give credit for success.\nAnother key idea is that leaders shape culture through their behavioural example — how they show up in meetings, how they react to bad news, how they listen (or don’t), how they respond under stress. You can’t fake your culture. If you preach values but act contrary to them, people will follow what you do, not what you say.\nFinally, Collins makes a clear distinction between leadership as personality versus leadership as responsibility. True leadership is not about style points — it’s about being the one who takes ownership of creating a culture of excellence and discipline, even when it’s hard or unpopular.\n\n\nChapter 4: Vision\n\nA vision is not what you hope to become. A vision is what you are willing to suffer for.\n\nThis chapter is the philosophical centre of Beyond Entrepreneurship 2.0. Collins argues that vision is not a statement — it’s a living system of meaning that drives everything in a company: decisions, culture, strategy, behaviour. A powerful vision doesn’t simply hang on a wall or get recited in meetings. It clarifies why your organisation exists, what you refuse to compromise, and what you’re daring to become.\nCollins breaks down vision into three essential components: core values, core purpose, and BHAGs (Big Hairy Audacious Goals). Core values are the non-negotiables — behaviours and principles that you hold onto even when they cost you. He’s quick to note that values don’t need to be universal; they just need to be authentic and lived. Some companies value innovation, others discipline, others humility. What matters is that the values are real and visible in daily actions.\nCore purpose, meanwhile, answers the deeper question: “Why do we exist?” It’s not about profit or growth — those are outcomes. Purpose is the organisation’s reason for being that goes beyond money. When a company loses its purpose, it may still function, but it becomes hollow. Collins compares this to a body without a soul: technically alive, but lacking spirit.\nFinally, BHAGs are long-term, ambitious goals — clear, compelling targets that inspire people to stretch far beyond incremental progress. A good BHAG is audacious but not reckless, emotionally engaging, and measurable enough to know when you’ve achieved it. Importantly, BHAGs work only when they’re grounded in a deep understanding of what your organisation can be great at.\nThroughout the chapter, Collins reinforces that vision isn’t about sounding good — it’s about alignment. The best companies have congruence between what they say, what they believe, and what they do. Vision becomes powerful when it’s operational — when hiring decisions, performance reviews, product choices, and even budget cuts are driven by it.\nThis chapter challenges leaders to get honest: Is your vision just branding language? Or is it a disciplining force in the organisation? Do your people know it? Live it? Care about it? If not, the problem isn’t the words — it’s the leadership behind them.\n\n\nChapter 5: Luck Favors the Persistent\n\nLuck is not a plan. Discipline is.\n\nIn this chapter, Collins tackles the seductive myth that success in business is mostly a matter of good luck — being in the right place at the right time, having the right investor, catching the right market wave. He doesn’t deny that luck exists. In fact, he admits that every great entrepreneur benefits from luck in some form. But he sharply distinguishes between getting luck and returning luck with discipline, preparation, and grit. What ultimately separates those who build enduring companies from those who don’t is not how much luck they get — it’s how they respond to the luck they receive.\nCollins introduces the idea of the “Return on Luck” (ROL) — a concept borrowed from Great by Choice. Two companies might receive the same stroke of luck (a booming market, a competitor imploding, a regulatory tailwind), but one seizes it and amplifies it through disciplined action, while the other squanders it or becomes complacent. In other words, luck is a variable — not a strategy. The entrepreneurs who endure are those who show up every day, stay the course during uncertainty, and are always ready to turn a lucky break into a breakthrough.\nThe chapter features examples of companies and founders who weren’t just lucky — they were relentless. They worked longer, studied harder, failed more often, and got back up faster than others. Collins makes it clear that luck can catalyse a result, but it’s persistence that compounds outcomes over time. He urges readers to build a culture of grit — one where effort, resilience, and preparation are valued as highly as intelligence or creativity.\nHe also issues a warning: people who attribute their failures to bad luck often lack the self-awareness or discipline to grow. Great leaders reflect on their own role in missed opportunities. They ask, “Did we prepare well enough?” or “Did we execute fast enough?” instead of blaming external forces. The best companies track their lucky and unlucky events — and how they responded — as part of their learning culture.\nUltimately, this chapter is about rejecting the lottery mentality and embracing the grind. Enduring greatness is rarely the result of one lucky moment. It is the result of ten thousand unsexy decisions, made with consistency, that put you in a position to benefit when luck finally knocks.\n\n\nChapter 6: What Makes Great Companies Tick – The Map\n\nThe great companies are not built by one thing done right — they are built by a system of interlocking concepts executed with relentless consistency.\n\nIn this pivotal chapter, Collins introduces what he calls The Map — a unifying framework that outlines the essential elements behind all great, enduring companies. The purpose of the chapter is to synthesis the ideas explored in earlier chapters into a coherent, visual and conceptual system. Collins wants readers to stop thinking in isolated traits or silver bullets, and instead understand how the pieces of greatness connect and reinforce each other.\nAt the centre of the map is the idea that greatness comes not from any single variable — not vision, not leadership, not innovation — but from the dynamic interplay of several foundational elements. These include: Level 5 Leadership, First Who, Then What, Confront the Brutal Facts, The Hedgehog Concept, A Culture of Discipline, The Flywheel, Preserve the Core / Stimulate Progress, and Clock Building. Each of these becomes a point on the map — not steps in a process, but interconnected disciplines that reinforce long-term excellence.\nWhat makes the map powerful is its universality. Collins argues that companies of vastly different industries, sizes, and stages can build greatness by adhering to this framework. The elements don’t rely on specific tactics, tools, or technologies. They are principles, and as such, they are durable. Moreover, they are cumulative — organisations don’t become great by mastering one of these principles. They become great by compounding them together, over time, with consistency.\nA central concept here is that of “clock building, not time telling” — the idea that great leaders focus on building enduring systems and cultures, rather than being individual geniuses with one great idea. Similarly, preserve the core and stimulate progress reminds readers that great companies hold tightly to their foundational values, but aggressively evolve everything else to remain relevant and effective.\nThe map is not a checklist or a how-to guide. It’s a way of thinking — a framework for diagnosing the health and direction of your organisation. Collins encourages leaders to ask: Where are we strong? Where are we weak? Which of these disciplines have we truly embedded? Which are only surface-level?\nUltimately, the map serves as a navigation tool — not for finding quick wins, but for building a company that can endure, adapt, and thrive for decades.\n\n\nChapter 7: Strategy\n\nA great strategy is as much about what you don’t do as what you choose to pursue.\n\nIn this chapter, Collins takes direct aim at the mystique of strategy as a high-concept, consultant-driven exercise. He argues that great strategy is not clever — it’s clear. It is not about crafting complex plans or elegant frameworks. Rather, it is about making deliberate choices, grounded in brutal facts, that align with your core values and long-term goals. The hallmark of a strong strategy is not how “smart” it looks on paper, but how actionable it is in practice — and how well it guides decision-making throughout the organisation.\nCollins re-emphasises the importance of what he calls the Hedgehog Concept, introduced in Good to Great. A great strategy sits at the intersection of three things: what you can be the best in the world at, what you are deeply passionate about, and what drives your economic engine. Companies that thrive in the long term are those that define this intersection with brutal honesty — and then commit to it with discipline. Those that fail often chase shiny objects, pivot endlessly, or confuse ambition with focus.\nA key insight in this chapter is that strategy is about saying no. Collins believes that most strategic failures are not due to poor thinking but to a lack of strategic constraint. In other words, companies get distracted. They pursue growth at the expense of coherence. A great strategy provides a filter for all decisions: new hires, product lines, acquisitions, partnerships. If an opportunity doesn’t reinforce the core strategic focus, it should be rejected — even if it looks profitable in the short term.\nHe also stresses the importance of empirical validation. Strategy should not be based on wishful thinking, but on observable evidence: where you actually perform best, what your customers truly value, and how your business model behaves under pressure. Vision without data is fantasy.\nFinally, Collins encourages leaders to codify their strategy into a form that everyone can understand — not just the executive team. When strategy is clear, it becomes a language within the company. People at all levels can make better decisions without constant oversight. And when the flywheel of execution begins to spin, it amplifies the force of strategic clarity.\n\n\nChapter 8: Innovation\n\nInnovation without discipline leads to disaster. Discipline without innovation leads to irrelevance.\n\nIn this chapter, Collins reframes innovation not as a burst of genius or an act of creative chaos, but as a disciplined, repeatable process. While innovation is essential to sustaining long-term greatness, Collins argues that it is not the starting point for most great companies. Innovation, when successful, is built upon a foundation of clarity — of purpose, values, and strategic focus — and then executed with rigour, not randomness.\nHe begins by dismantling a popular misconception: that the most innovative companies are those that generate the most ideas. In reality, innovation is not about the quantity of ideas but the quality of disciplined experimentation. Collins returns to the concept introduced earlier — “try a lot of stuff and keep what works” — but now applies it in a more mature strategic context. At the heart of this disciplined innovation process is what he calls the “bullets before cannonballs” approach.\nThis metaphor is one of the strongest in the book. It describes how great companies fire low-cost, low-risk bullets — small, testable initiatives — to calibrate aim. Once a bullet hits its target (meaning the idea shows empirical success), they fire a cannonball — scaling up with concentrated resources and confidence. Poorly led companies reverse this sequence. They bet big on untested ideas, wasting time, capital, and often their reputation.\nCollins makes it clear that innovation is not only about new products or technologies. It can be process innovation, business model innovation, cultural innovation — any deliberate change that improves the organisation’s performance or resilience. He also warns against “innovation-as-ego” — when leaders pursue novelty for novelty’s sake, or when they define themselves by their cleverness instead of their results.\nPerhaps most importantly, he emphasises that innovation must serve — not replace — the flywheel. Great companies innovate in ways that amplify what already works, not disrupt themselves with every new fad. Innovation is not the engine. It’s a force-multiplier once your engine is already running well.\nThis chapter ultimately calls for a blend of creativity and discipline. Breakthroughs don’t emerge from chaos; they emerge from well-calibrated curiosity, aligned with strategy and supported by data.\n\n\nChapter 9: Tactical Excellence\n\nThere is nothing more strategic than flawless execution.\n\nIn this chapter, Collins addresses a theme that’s often neglected in leadership books: the power of executional rigour. While vision, strategy, and innovation get most of the attention, what truly distinguishes enduring companies is their obsession with doing the small things well — over and over again. Tactical excellence, in Collins’s view, is not glamorous. It is not about inspiration. It is about getting the right things done the right way — consistently, at scale.\nHe challenges the false dichotomy between “visionary thinking” and “operational detail,” arguing that great leaders and organisations don’t get to choose between them. They do both. Tactical strength isn’t just for the operations team or middle management — it is a strategic advantage. Companies that execute with discipline gain speed, build trust, and outlast competitors who may have better ideas but weaker delivery.\nOne of the key points in this chapter is that execution is not just about processes and procedures; it’s about culture. In high-performing organisations, discipline becomes habit, and executional excellence becomes a shared norm. People take pride in their work, not because they’re told to, but because it’s who they are as a team. Collins often points to military analogies — elite units don’t succeed through creativity alone, but through flawless execution of well-rehearsed actions, even under pressure.\nHe also explores how sloppiness at the tactical level erodes trust at the leadership level. If leaders tolerate inconsistency in how meetings are run, how feedback is given, how decisions are followed through, they create confusion — and that confusion scales. Tactical sloppiness sends a signal: we are undisciplined. Eventually, that signal becomes cultural identity.\nAnother insight is the idea that discipline and bureaucracy are not the same. Collins is careful to distinguish the two. Bureaucracy arises when people are forced to follow unnecessary rules. Discipline, by contrast, arises when people commit voluntarily to doing what’s right, even when it’s hard or repetitive. The goal is not mindless compliance, but a shared commitment to operational excellence.\nThe chapter closes with a warning and an opportunity: if you don’t build tactical excellence early, you will eventually be forced to — usually under stress, pressure, or crisis. But if you embed it in your culture from the beginning, it becomes a quiet engine of resilience and credibility."
  },
  {
    "objectID": "posts/2025-09-01-hello.html",
    "href": "posts/2025-09-01-hello.html",
    "title": "Hello, world",
    "section": "",
    "text": "First post! I’ll use this space for short updates, notes, and news."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "What Guides My Work Numbers are easy. Uncertainty is the real challenge. A model is never just numbers on a page. It is a story about uncertainty, a reflection of choices, and a map of how we believe the world works. The hardest part of risk is not predicting the future, but deciding what kind of future we are willing to prepare for. — Vahab Rostampour, Ph.D.  \n\n\n📌 Recent updates, notes, & news →"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Home",
    "section": "About me",
    "text": "About me\nI am a researcher, quantitative analyst, and entrepreneur with a passion for solving complex problems at the intersection of mathematics, technology, and society. My background spans both academic research and financial industry practice, giving me a unique perspective on how rigorous theory can be transformed into practical solutions for real-world challenges.\nI hold a PhD in Applied Mathematics and Control Systems Engineering from Delft University of Technology, where I focused on optimisation, stochastic modelling, and distributed control of large-scale systems such as energy and infrastructure networks. After my PhD, I continued as a Postdoctoral Researcher at the University of Groningen, where my work centred on advanced methods in optimisation, learning, and decision-making in uncertain environments.\nIn parallel to my academic career, I have spent the last five years as a Senior Quantitative Analyst in banking, working at leading financial institutions in Zürich and Amsterdam. My focus has been on model risk management, credit and impairment modelling, and regulatory compliance frameworks such as IFRS 9, CECL, and stress testing. This experience has strengthened my ability to build bridges between advanced mathematical models, regulatory expectations, and practical business needs.\nToday, my research and professional interests converge around stochastic modelling, optimisation, and risk management across domains such as energy systems, financial markets, and infrastructure planning. I am particularly driven by the question of how mathematical models, data-driven methods, and economic principles can be combined to design robust, transparent, and scalable decision frameworks for uncertain and dynamic environments.\nBeyond my professional work, I enjoy building new ideas into projects, from smart energy applications for electric vehicles to risk and pricing frameworks for decentralised finance. At heart, I am motivated by curiosity, collaboration, and the ambition to create tools that not only solve technical problems but also contribute to more resilient systems for the future."
  },
  {
    "objectID": "index.html#highlights",
    "href": "index.html#highlights",
    "title": "Home",
    "section": "Highlights",
    "text": "Highlights\n\nScenario Convexification of Nonconvex Uncertain Programs\n\nMy recent work introduces convexification techniques that bridge theory and computation in nonconvex uncertain optimisation problems. The approach demonstrates how scenario-based approximations can make otherwise intractable models both solvable and practically relevant. This research contributes to the understanding of duality gaps, tractability, and robust decision-making under uncertainty — with applications in portfolio selection and large-scale systems.\n\nLifetime Probability of Default and Kalman Filtering Approaches\n\nIn financial risk management, I have developed methodologies for modelling lifetime probability of default (PD), incorporating Kalman filtering techniques to enhance estimation under uncertainty. This work addresses regulatory requirements (IFRS 9, CECL) while providing more stable and transparent credit risk measures. It reflects my broader interest in connecting stochastic state-space models with practical risk assessment frameworks used in banking.\n\nBook Project: Quantitative Interview Questions (QIQ)\n\nI am authoring a book that brings together quantitative finance, probability, and statistics into a practical guide for professionals preparing for technical interviews. The book combines rigorous exercises with clear explanations, covering topics from risk modelling and stochastic processes to applied coding tasks. My aim is to create not just a collection of questions, but a structured learning resource that builds both technical mastery and intuition."
  },
  {
    "objectID": "index.html#outreach",
    "href": "index.html#outreach",
    "title": "Home",
    "section": "Outreach",
    "text": "Outreach\nI am always open to collaboration, discussion, and exchange of ideas across academia, industry, and entrepreneurship. Whether you are a researcher, practitioner, or simply curious about topics such as stochastic modelling, optimisation, energy systems, or financial risk management, I welcome opportunities to connect.\nI also enjoy engaging in talks, panel discussions, and podcasts, where complex ideas can be translated into accessible conversations that reach a broader audience. If you would like to explore a joint project, invite me for a speaking opportunity, or simply exchange perspectives, please feel free to reach out directly to me!"
  },
  {
    "objectID": "index.html#policies",
    "href": "index.html#policies",
    "title": "Home",
    "section": "Policies",
    "text": "Policies\nThis website is equipped with the following policies:\n\nCopyright\nAll materials on this website are made available for the timely dissemination of scholarly and technical work. Copyright and all rights remain with the original authors or copyright holders. Visitors are expected to respect these rights and comply with the terms of each copyright.\nPersonal use of the material is permitted. Any reproduction, redistribution, or republication of material for commercial purposes, advertising, promotional use, or inclusion in collective works requires prior written permission from the copyright holder.\n\n\nDisclaimer\nThe information on this website is provided as is without warranty of any kind. No responsibility or liability is accepted for any loss or damage that may result from reliance on the content of this website.\nThis website may contain links to external sites. These links are provided for convenience only, and no responsibility is assumed for the content or reliability of external websites.\n\n\nPrivacy Issues\nThis website does not use cookies and does not require you to provide personal information to access its content. If you choose to contact me directly, any personal information you provide will be treated with care and will not be shared without your consent."
  },
  {
    "objectID": "posts/2025-09-02-publications.html",
    "href": "posts/2025-09-02-publications.html",
    "title": "Publication page is ready!",
    "section": "",
    "text": "I’ve just refreshed my Publications page — it’s now fully up to date with my recent journal articles, conference papers, and book contributions.\nThe list is organised by category and year, with the most recent work at the top. My aim is to make it easier for colleagues, collaborators, and students to browse my research across energy systems, optimisation, and quantitative risk management."
  },
  {
    "objectID": "posts/2025-09-05-ZeroOne.html",
    "href": "posts/2025-09-05-ZeroOne.html",
    "title": "Book Reading Summary – Zero to One",
    "section": "",
    "text": "Zero to One is a contrarian guide to startups, progress, and the mechanics of building something radically new. Drawing from his experiences as co-founder of PayPal and early investor in Facebook and Palantir, Peter Thiel outlines how entrepreneurs should think — not just execute.\nHe argues that true innovation happens when you go from nothing to something — not by copying others or making incremental improvements, but by discovering unique insights and building monopolies around them.\n\nDoing what we already know how to do takes the world from 1 to n. But doing something truly new takes it from 0 to 1."
  },
  {
    "objectID": "posts/2025-09-05-ZeroOne.html#core-idea",
    "href": "posts/2025-09-05-ZeroOne.html#core-idea",
    "title": "Book Reading Summary – Zero to One",
    "section": "",
    "text": "Zero to One is a contrarian guide to startups, progress, and the mechanics of building something radically new. Drawing from his experiences as co-founder of PayPal and early investor in Facebook and Palantir, Peter Thiel outlines how entrepreneurs should think — not just execute.\nHe argues that true innovation happens when you go from nothing to something — not by copying others or making incremental improvements, but by discovering unique insights and building monopolies around them.\n\nDoing what we already know how to do takes the world from 1 to n. But doing something truly new takes it from 0 to 1."
  },
  {
    "objectID": "posts/2025-09-05-ZeroOne.html#personal-frank-view",
    "href": "posts/2025-09-05-ZeroOne.html#personal-frank-view",
    "title": "Book Reading Summary – Zero to One",
    "section": "Personal Frank View",
    "text": "Personal Frank View\nThis book is a sharp, ambitious, and deeply opinionated book. It’s not a startup manual. it’s a startup manifesto. You should not read it for step-by-step guidance, but to sharpen how you think about value, progress, and the future. Importantly, it is essential reading if you’re building something new, but it is dangerous if taken as universal truth."
  },
  {
    "objectID": "posts/2025-09-05-ZeroOne.html#chapters-summary",
    "href": "posts/2025-09-05-ZeroOne.html#chapters-summary",
    "title": "Book Reading Summary – Zero to One",
    "section": "Chapters Summary",
    "text": "Chapters Summary\n\nChapter 1: The Challenge of the Future\n\nBrilliant thinking is rare, but courage is in even shorter supply than genius.\n\nThis chapter opens with the book’s central thesis: the future isn’t inevitable — we can shape it. Thiel introduces the distinction between horizontal progress (going from 1 to n, copying what works) and vertical progress (going from 0 to 1, doing something truly new). He argues that while globalisation is valuable, it’s not enough — the real breakthroughs come from technology and original thinking. The most important task of the 21st-century entrepreneur is to discover and create something new. Innovation is rare, but it’s the only way to build a better future.\n\n\nChapter 2: Party Like It’s 1999\n\nLeanness is a methodology, not a goal. Boldness builds value.\n\nThiel revisits the dot-com crash and critiques the tech bubble mindset. He explains how four dangerous lessons were mistakenly learned by that generation of founders:\n\nMake incremental advances.\nStay lean and flexible.\nImprove on the competition.\nFocus on product, not sales.\n\nHe argues these became dogma — but they are often wrong. Instead, Thiel encourages founders to build bold visions, avoid direct competition, and aim for monopoly. Playing it safe is the riskiest strategy of all.\n\n\nChapter 3: All Happy Companies Are Different\n\nAll failed companies are the same: they failed to escape competition.\n\nIn this chapter, Thiel presents his most famous and provocative idea:\n\n“Competition is for losers.”\n\nGreat businesses are monopolies — they do something so well that no one else can offer a close substitute. Bad businesses fight to survive in commoditised markets. He explains that monopolies aren’t evil — they enable companies to focus on long-term innovation and customer value. Meanwhile, competitive businesses are too busy fighting over scraps to innovate meaningfully.\n\n\nChapter 4: The Ideology of Competition\n\nIf you can recognize competition as a destructive force instead of a sign of value, you’re already more sane than most.\n\nThiel explores how society idolises competition — in schools, sports, and business — despite its destructive consequences. He argues that competition narrows thinking, distorts incentives, and causes people to chase validation rather than value. Entrepreneurs, in contrast, must learn to ignore competitors and focus on building something no one else can. If you’re constantly reacting to others, you’re not leading — you’re following.\n\n\nChapter 5: Last Mover Advantage\n\nThe most successful companies make the last great development in a specific market — and enjoy decades of monopoly profits.\n\nHere Thiel challenges the idea that first-mover advantage is crucial. In fact, being first is not what matters — it’s being the last company to dominate a space. He outlines four key characteristics of lasting monopolies:\n\nProprietary technology\nNetwork effects\nEconomies of scale\nBranding\n\nFounders should think strategically about how to scale defensibly and become the “last mover” — the company that owns the market, not just enters it.\n\n\nChapter 6: You Are Not a Lottery Ticket\n\nIndefinite attitudes to the future explain what’s most dysfunctional in our world today.\n\nThiel criticises the popular belief that success is mostly about chance. He promotes a worldview of definite optimism — the belief that the future can be planned and shaped. He contrasts this with indefinite optimism (hope without a plan), definite pessimism (planning for decline), and indefinite pessimism (drift and cynicism). The best founders reject randomness. They plan, commit, and build with purpose.\n\n\nChapter 7: Follow the Money\n\nThe biggest secret in venture capital is that the best investment in a successful fund equals or outperforms the entire rest of the fund combined.\n\nThis chapter explains the power law in startups — the idea that a tiny number of companies generate nearly all the returns. Whether in venture capital, personal careers, or startup impact, the rule holds: most things fail, and the few successes dominate. Thiel urges founders and investors to focus obsessively on the one thing that could yield outsized results. Diversification, he says, is often just disguised ignorance.\n\n\nChapter 8: Secrets\n\nThe best problems to work on are often the ones nobody else even tries to solve.\n\nThiel believes there are still undiscovered truths about the world — what he calls “secrets.” The key to entrepreneurship is finding something valuable that nobody else sees. Thiel splits secrets into secrets of nature (scientific or technical) and secrets of people (hidden social insights). Conventional wisdom says everything has been discovered. Great founders reject that. They look where others aren’t looking and ask: What valuable company is nobody building?\n\n\nChapter 9: Foundations\n\nA startup messed up at its foundation cannot be fixed.\n\nThis chapter deals with the most overlooked part of a startup: founding decisions. Equity splits, culture, cofounder alignment — all these are critical, and mistakes here are very hard to fix later. Thiel stresses founder unity, clarity of vision, and tightly aligned incentives. A weak foundation, even with a good idea, often leads to conflict, stagnation, or collapse.\n\n\nhapter 10: The Mechanics of Mafia\n\nYou can’t outsource culture. You must build it — brick by brick, belief by belief.\n\nThiel describes how successful startups operate like tight-knit mafias — intensely mission-driven teams with shared values and long-term loyalty. He contrasts this with bureaucratic corporations, where people clock in and out without connection. The goal is to build a team that believes in the vision so strongly, they’d rather work there than anywhere else. Culture, he argues, is not a soft asset — it’s a competitive advantage.\n\n\nChapter 11: If You Build It, Will They Come?\n\nSuperior sales and distribution by itself can create a monopoly, even with no product differentiation.\n\nHere Thiel dismantles the myth that “great products sell themselves.” They don’t. Every startup needs a distribution strategy — a way to reach customers. He categorises sales into complex, personal, automated, and viral channels, explaining which work for which kinds of products. Without distribution, even the best technology fails. “Poor distribution — not poor product — is the number one cause of failure.”\n\n\nChapter 12: Man and Machine\n\nBetter technology lets people do more, not less.\n\nThis chapter responds to fears about artificial intelligence. Thiel rejects the “man vs machine” narrative and proposes a man + machine model. The best systems amplify human strengths, rather than replace them. He cites Palantir as an example — software that helps humans make better decisions, rather than making decisions for them. The best startups build complementary intelligence, not competition.\n\n\nChapter 13: Seeing Green\n\nThe seven questions every business must answer are unforgiving — ignore even one, and you’re likely to fail.\n\nIn this case study chapter, Thiel analyses the clean tech bubble and why most green startups failed. He identifies seven questions every startup must answer:\n\nEngineering\nTiming\nMonopoly\nPeople\nDistribution\nDurability\nSecret\n\nCleantech companies ignored many of these — especially monopoly and distribution — and paid the price. This framework is a diagnostic tool for judging startup viability.\n\n\nChapter 14: The Founder’s Paradox\n\nA unique founder can make a unique company. But uniqueness is a double-edged sword.\n\nIn the final chapter, Thiel wrestles with the idea that founders often need to be extreme personalities. Society wants balance, but transformative companies are usually built by unbalanced, intense, eccentric individuals. This is a paradox: the very traits that make founders dangerous also make them powerful. The challenge is to channel extremism into long-term impact — without burning everything down."
  },
  {
    "objectID": "posts/anchored-kf.html",
    "href": "posts/anchored-kf.html",
    "title": "Anchored Kalman Filtering in a Markov PD Model — Reproducible Demo",
    "section": "",
    "text": "This page provides a reproducible environment for anchored Kalman filtering inside a Markov credit rating model.\n\nMinimal stack: NumPy, Pandas, Matplotlib.\nEach section explains the code, runs it, shows the result inline, and saves files under outputs/.\nScenarios: baseline, stress, pandemic."
  },
  {
    "objectID": "posts/anchored-kf.html#overview",
    "href": "posts/anchored-kf.html#overview",
    "title": "Anchored Kalman Filtering in a Markov PD Model — Reproducible Demo",
    "section": "",
    "text": "This page provides a reproducible environment for anchored Kalman filtering inside a Markov credit rating model.\n\nMinimal stack: NumPy, Pandas, Matplotlib.\nEach section explains the code, runs it, shows the result inline, and saves files under outputs/.\nScenarios: baseline, stress, pandemic."
  },
  {
    "objectID": "posts/anchored-kf.html#imports-paths",
    "href": "posts/anchored-kf.html#imports-paths",
    "title": "Anchored Kalman Filtering in a Markov PD Model — Reproducible Demo",
    "section": "0) Imports & Paths",
    "text": "0) Imports & Paths\nWe import our minimal stack and set a portable output directory.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple\nimport zlib  # stable, deterministic offsets (better than built-in hash)\n\n# Output directory (portable for local/Thebe/Binder)\nOUTDIR = Path(\"outputs\")\nOUTDIR.mkdir(parents=True, exist_ok=True)\nOUTDIR.resolve()"
  },
  {
    "objectID": "posts/anchored-kf.html#universe-determinism",
    "href": "posts/anchored-kf.html#universe-determinism",
    "title": "Anchored Kalman Filtering in a Markov PD Model — Reproducible Demo",
    "section": "1) Universe & Determinism",
    "text": "1) Universe & Determinism\nWe define the rating states and a deterministic RNG helper.\n\nRATINGS = [\"A\", \"B\", \"C\", \"D\"]\nIDX = {r: i for i, r in enumerate(RATINGS)}\nT_DEFAULT = 20    # quarters\nN_DEFAULT = 10_000\n\ndef set_seed(seed: int) -&gt; np.random.Generator:\n    \"\"\"Set global determinism and return a local RNG.\"\"\"\n    np.random.seed(seed)\n    return np.random.default_rng(seed)\n\n# Demo: create a reproducible RNG\nrng = set_seed(42)\nrng"
  },
  {
    "objectID": "posts/anchored-kf.html#portfolio-initialisation",
    "href": "posts/anchored-kf.html#portfolio-initialisation",
    "title": "Anchored Kalman Filtering in a Markov PD Model — Reproducible Demo",
    "section": "2) Portfolio Initialisation",
    "text": "2) Portfolio Initialisation\nWe sample initial ratings for N obligors from a user-supplied cross-sectional distribution.\n\ndef gen_initial_portfolio(N: int, pi0: np.ndarray, rng: np.random.Generator=None) -&gt; np.ndarray:\n    \"\"\"Sample initial ratings for N obligors from distribution pi0.\"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n    labels = rng.choice(len(RATINGS), size=N, p=pi0)\n    return labels\n\n# Demo: quick cross-section snapshot\npi0_demo = np.array([0.50, 0.35, 0.10, 0.05], dtype=float)\nlabels0_demo = gen_initial_portfolio(N_DEFAULT, pi0_demo, rng=rng)\n(pd.Series(labels0_demo)\n   .map({i:r for r,i in IDX.items()})\n   .value_counts(normalize=True)\n   .rename(\"share\")\n   .to_frame()\n   .sort_index())"
  },
  {
    "objectID": "posts/anchored-kf.html#through-the-cycle-ttc-transition-matrix",
    "href": "posts/anchored-kf.html#through-the-cycle-ttc-transition-matrix",
    "title": "Anchored Kalman Filtering in a Markov PD Model — Reproducible Demo",
    "section": "3) Through-the-Cycle (TTC) Transition Matrix",
    "text": "3) Through-the-Cycle (TTC) Transition Matrix\nWe use a quarterly TTC matrix with D absorbing.\n\ndef build_P_TTC() -&gt; np.ndarray:\n    \"\"\"Return the quarterly TTC transition matrix.\"\"\"\n    P_TTC = np.array([\n        [0.975, 0.022, 0.002, 0.001],\n        [0.030, 0.935, 0.030, 0.005],\n        [0.010, 0.060, 0.915, 0.015],\n        [0.000, 0.000, 0.000, 1.000],\n    ], dtype=float)\n    return P_TTC\n\nP_TTC = build_P_TTC()\n# Verify rows sum to 1\nassert np.allclose(P_TTC.sum(axis=1), 1.0)\nP_TTC\n\nVisualise & save\n\nfig, ax = plt.subplots(figsize=(4.5, 3.5))\nim = ax.imshow(P_TTC, aspect=\"auto\")\nax.set_xticks(range(len(RATINGS))); ax.set_xticklabels(RATINGS)\nax.set_yticks(range(len(RATINGS))); ax.set_yticklabels(RATINGS)\nax.set_title(\"Quarterly TTC Transition Matrix\")\nfig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\nfpath = OUTDIR / \"ttc_matrix.png\"\nfig.savefig(fpath, bbox_inches=\"tight\")\nplt.show()\nfpath"
  },
  {
    "objectID": "posts/anchored-kf.html#macroeconomic-scenarios-index",
    "href": "posts/anchored-kf.html#macroeconomic-scenarios-index",
    "title": "Anchored Kalman Filtering in a Markov PD Model — Reproducible Demo",
    "section": "4) Macroeconomic Scenarios & Index",
    "text": "4) Macroeconomic Scenarios & Index\nWe define three stylised macro forecasts and simulate realisations by adding noise. We also build a macro index \\(M_t = 0.5\\,z(\\text{GDP}_t) - 0.5\\,z(\\text{UNEMP}_t)\\).\n\ndef _baseline_paths(T: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n    t = np.arange(T)\n    gdp = 0.5 + 0.3*np.sin(2*np.pi*t/12)\n    unemp = 5.0 + 0.2*np.cos(2*np.pi*(t+3)/10)\n    return gdp, unemp\n\ndef _stress_paths(T: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n    t = np.arange(T)\n    gdp = 0.6 - 0.8*np.exp(-t/2.0)\n    gdp[:4] -= 0.8\n    unemp = 4.5 + 1.8*(1 - np.exp(-t/4.0))\n    return gdp, unemp\n\ndef _pandemic_paths(T: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n    t = np.arange(T)\n    gdp = 0.5*np.ones(T)\n    gdp[0:2] = -2.0\n    gdp[2] = -0.5\n    unemp = 4.2*np.ones(T)\n    unemp[0:3] = 5.5\n    unemp[3:6] = 5.0\n    return gdp, unemp\n\ndef macro_index_from_gdp_unemp(GDP: np.ndarray, UNEMP: np.ndarray) -&gt; np.ndarray:\n    def z(x: np.ndarray) -&gt; np.ndarray:\n        mu = np.mean(x); sigma = np.std(x, ddof=0); sigma = sigma if sigma &gt; 1e-12 else 1.0\n        return (x - mu) / sigma\n    return 0.5*z(GDP) - 0.5*z(UNEMP)\n\ndef gen_macro_forecasts_and_realised(scenario: str, T: int=T_DEFAULT, rng: np.random.Generator=None) -&gt; Dict[str, np.ndarray]:\n    if rng is None:\n        rng = np.random.default_rng()\n    sc = scenario.lower()\n    if sc == \"baseline\":\n        gdp_f, unemp_f = _baseline_paths(T)\n    elif sc == \"stress\":\n        gdp_f, unemp_f = _stress_paths(T)\n    elif sc in {\"pandemic\", \"pandemic_shock\"}:\n        gdp_f, unemp_f = _pandemic_paths(T)\n    else:\n        raise ValueError(\"Unknown scenario\")\n\n    # Realised = forecast + Gaussian noise\n    gdp_r = gdp_f + rng.normal(0.0, 0.2, size=T)\n    unemp_r = unemp_f + rng.normal(0.0, 0.2, size=T)\n\n    # Pandemic rebound at t=3\n    if sc in {\"pandemic\", \"pandemic_shock\"} and T &gt;= 4:\n        gdp_r[3] += 2.5\n        unemp_r[3] -= 0.8\n\n    M_hat = macro_index_from_gdp_unemp(gdp_f, unemp_f)\n    M_real = macro_index_from_gdp_unemp(gdp_r, unemp_r)\n    return {\"gdp_forecast\": gdp_f, \"unemp_forecast\": unemp_f,\n            \"gdp_realised\": gdp_r, \"unemp_realised\": unemp_r,\n            \"M_hat\": M_hat, \"M_real\": M_real}\n\nQuick visual check\n\nscenarios = [\"baseline\", \"stress\", \"pandemic\"]\nfig, axes = plt.subplots(3, 2, figsize=(10, 8), sharex=True)\nfor i, sc in enumerate(scenarios):\n    res = gen_macro_forecasts_and_realised(sc, T=20, rng=rng)\n    axes[i,0].plot(res[\"gdp_forecast\"], label=\"Forecast\"); axes[i,0].plot(res[\"gdp_realised\"], label=\"Realised\", alpha=0.7)\n    axes[i,0].set_title(f\"{sc.title()} GDP\"); axes[i,0].legend()\n    axes[i,1].plot(res[\"unemp_forecast\"], label=\"Forecast\"); axes[i,1].plot(res[\"unemp_realised\"], label=\"Realised\", alpha=0.7)\n    axes[i,1].set_title(f\"{sc.title()} Unemployment\")\nplt.tight_layout()\nfpath = OUTDIR / \"macro_scenarios_overview.png\"\nplt.savefig(fpath, bbox_inches=\"tight\"); plt.show(); fpath"
  },
  {
    "objectID": "posts/anchored-kf.html#pit-overlay-logit-tilt",
    "href": "posts/anchored-kf.html#pit-overlay-logit-tilt",
    "title": "Anchored Kalman Filtering in a Markov PD Model — Reproducible Demo",
    "section": "5) PIT Overlay (Logit Tilt)",
    "text": "5) PIT Overlay (Logit Tilt)\nWe tilt TTC with a \\(\\beta\\) matrix using \\(W = P \\odot \\exp(\\beta \\cdot M_t)\\), then row-normalise (and keep D absorbing).\n\ndef _build_betas() -&gt; np.ndarray:\n    beta = np.zeros((4,4), dtype=float)\n    beta[IDX[\"A\"], IDX[\"B\"]] = 2.0; beta[IDX[\"A\"], IDX[\"C\"]] = 2.5; beta[IDX[\"A\"], IDX[\"D\"]] = 3.0\n    beta[IDX[\"B\"], IDX[\"C\"]] = 1.5; beta[IDX[\"B\"], IDX[\"D\"]] = 2.0\n    beta[IDX[\"C\"], IDX[\"D\"]] = 1.2\n    # upgrades negative\n    beta[IDX[\"B\"], IDX[\"A\"]] = -beta[IDX[\"A\"], IDX[\"B\"]]\n    beta[IDX[\"C\"], IDX[\"B\"]] = -beta[IDX[\"B\"], IDX[\"C\"]]\n    beta[IDX[\"C\"], IDX[\"A\"]] = -beta[IDX[\"A\"], IDX[\"C\"]]\n    return beta\n\ndef pit_overlay(P_TTC: np.ndarray, M_t: float, betas: np.ndarray) -&gt; np.ndarray:\n    P = np.array(P_TTC, dtype=float)\n    W = P * np.exp(betas * M_t)\n    for i in range(W.shape[0]):\n        if i == IDX[\"D\"]:\n            W[i, :] = 0.0; W[i, IDX[\"D\"]] = 1.0\n        else:\n            s = W[i, :].sum()\n            if s &lt;= 0 or not np.isfinite(s): W[i, :] = P[i, :]\n            else: W[i, :] = W[i, :] / s\n    return W\n\n# Demo: effect of negative vs positive M on an A-row slice\nbetas = _build_betas()\nmvals = [-1.0, 0.0, 1.0]\nfig, ax = plt.subplots(figsize=(6,3))\nfor mv in mvals:\n    W = pit_overlay(P_TTC, mv, betas)\n    ax.plot(W[IDX[\"A\"], :], marker=\"o\", label=f\"M={mv:+.1f}\")\nax.set_xticks(range(4)); ax.set_xticklabels(RATINGS); ax.set_title(\"A-row PIT probabilities vs macro index\")\nax.legend(); fpath = OUTDIR / \"pit_row_A_vs_M.png\"; plt.savefig(fpath, bbox_inches=\"tight\"); plt.show(); fpath"
  },
  {
    "objectID": "posts/anchored-kf.html#kalman-filters-naïve-vs-anchored",
    "href": "posts/anchored-kf.html#kalman-filters-naïve-vs-anchored",
    "title": "Anchored Kalman Filtering in a Markov PD Model — Reproducible Demo",
    "section": "6) Kalman Filters (Naïve vs Anchored)",
    "text": "6) Kalman Filters (Naïve vs Anchored)\nWe compare a naïve KF \\(y_t = M^{\\hat{}}_t\\) to an anchored KF with a second observation (anchor at 0), soft before \\(T_{\\text{anchor}}\\) and hard after.\n\ndef kalman_naive(M_hat: np.ndarray, rho: float=0.90, Q: float=None, R: float=0.25) -&gt; np.ndarray:\n    T = len(M_hat)\n    if Q is None: Q = 1 - rho**2\n    m, P, H = 0.0, 1.0, 1.0\n    m_filt = np.zeros(T)\n    for t in range(T):\n        m_pred = rho * m; P_pred = rho * P * rho + Q\n        y = M_hat[t]; S = H * P_pred * H + R; K = P_pred * H / S\n        m = m_pred + K * (y - H * m_pred); P = (1 - K * H) * P_pred\n        m_filt[t] = m\n    return m_filt\n\ndef kalman_anchored(M_hat: np.ndarray, T_anchor: int=20, rho: float=0.90, Q: float=None, R: float=0.25, sigma_star2_pre: float=0.25) -&gt; np.ndarray:\n    T = len(M_hat)\n    if Q is None: Q = 1 - rho**2\n    m, P = 0.0, 1.0\n    H = np.array([[1.0],[1.0]])  # (2,1)\n    m_filt = np.zeros(T)\n    for t in range(T):\n        m_pred = rho * m; P_pred = rho * P * rho + Q\n        y = np.array([M_hat[t], 0.0])\n        R_aug = np.diag([R, sigma_star2_pre]) if t &lt; T_anchor else np.diag([R, 1e-12])\n        S = H @ np.array([[P_pred]]) @ H.T + R_aug\n        K = (np.array([[P_pred]]) @ H.T) @ np.linalg.solve(S, np.eye(2))\n        innov = y - (H[:,0]*m_pred)\n        m = m_pred + (K @ innov)[0]; P = (1 - (K @ H)[0,0]) * P_pred\n        m_filt[t] = m\n    return m_filt\n\n# Demo: compare filters on baseline forecast index\ndemo = gen_macro_forecasts_and_realised(\"baseline\", T=40, rng=rng)\nMhat_demo = demo[\"M_hat\"]\nm_naive = kalman_naive(Mhat_demo, rho=0.9, R=0.25)\nm_anchor = kalman_anchored(Mhat_demo, T_anchor=20, rho=0.9, R=0.25, sigma_star2_pre=0.25)\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.plot(Mhat_demo, label=\"Forecast index  $M^\\\\hat{}_t$\", alpha=0.6)\nax.plot(m_naive, label=\"Naïve KF  $m_t$\")\nax.plot(m_anchor, label=\"Anchored KF  $m_t$\")\nax.axvline(20, ls=\"--\", alpha=0.5, label=\"Anchor switch\")\nax.set_title(\"Naïve vs Anchored Kalman Filter (Baseline)\")\nax.legend(); fpath = OUTDIR / \"kf_naive_vs_anchored.png\"; plt.savefig(fpath, bbox_inches=\"tight\"); plt.show(); fpath"
  },
  {
    "objectID": "posts/anchored-kf.html#distribution-propagation-pd-series",
    "href": "posts/anchored-kf.html#distribution-propagation-pd-series",
    "title": "Anchored Kalman Filtering in a Markov PD Model — Reproducible Demo",
    "section": "7) Distribution Propagation & PD Series",
    "text": "7) Distribution Propagation & PD Series\nWe propagate \\(\\pi_t = \\pi_{t-1} P_t\\) and extract default mass \\(Y_t = \\pi_t[D]\\).\n\ndef propagate_distribution(pi0: np.ndarray, P_ts: List[np.ndarray]) -&gt; np.ndarray:\n    T = len(P_ts); pi = np.zeros((T+1, len(RATINGS))); pi[0, :] = pi0; current = pi0.copy()\n    for t in range(T):\n        current = current @ P_ts[t]; pi[t+1, :] = current\n    return pi\n\ndef compute_pd_series(pi_ts: np.ndarray) -&gt; np.ndarray:\n    return pi_ts[:, IDX[\"D\"]]\n\n# Demo: one-quarter step under TTC from the earlier labels cross-section\nP1 = [P_TTC.copy()]\npi_one = propagate_distribution(pi0_demo, P1)\npd.Series(pi_one[-1], index=RATINGS).rename(\"π_1\")"
  },
  {
    "objectID": "posts/anchored-kf.html#experiment-runner-scenario-method",
    "href": "posts/anchored-kf.html#experiment-runner-scenario-method",
    "title": "Anchored Kalman Filtering in a Markov PD Model — Reproducible Demo",
    "section": "8) Experiment Runner (Scenario × Method)",
    "text": "8) Experiment Runner (Scenario × Method)\nWe wrap the pipeline and save CSVs + a macro figure per run.\n\ndef _stable_offset(scenario: str, method: str, mod: int = 10_000) -&gt; int:\n    key = f\"{scenario}|{method}\".encode(); return zlib.crc32(key) % mod\n\ndef run_experiment(scenario: str, method: str, N: int=N_DEFAULT, T: int=T_DEFAULT, seed: int=12345) -&gt; Dict[str, object]:\n    rng = set_seed(seed + _stable_offset(scenario, method))\n    pi0 = np.array([0.45, 0.40, 0.15, 0.00], dtype=float)\n    labels0 = gen_initial_portfolio(N, pi0, rng=rng)\n    P_TTC = build_P_TTC(); betas = _build_betas()\n\n    macro = gen_macro_forecasts_and_realised(scenario, T=T, rng=rng)\n    M_hat, M_real = macro[\"M_hat\"], macro[\"M_real\"]\n\n    if method == \"raw\":\n        M_est = M_real.copy()\n    elif method == \"naive\":\n        M_est = kalman_naive(M_hat, rho=0.90, Q=None, R=0.25)\n    elif method == \"anchored\":\n        M_est = kalman_anchored(M_hat, T_anchor=T, rho=0.90, Q=None, R=0.25, sigma_star2_pre=0.25)\n    else:\n        raise ValueError(\"Unknown method\")\n\n    P_ts = [pit_overlay(P_TTC, float(M_est[t]), betas) for t in range(T)]\n    pi_ts = propagate_distribution(pi0, P_ts); Y_t = compute_pd_series(pi_ts)\n\n    P_neutral = [P_TTC.copy() for _ in range(T)]\n    pi_ttc = propagate_distribution(pi0, P_neutral); Y_ttc = compute_pd_series(pi_ttc)\n\n    # Save transition matrices\n    tm_rows = []\n    for t in range(T):\n        for i, ri in enumerate(RATINGS):\n            for j, rj in enumerate(RATINGS):\n                tm_rows.append({\"t\": t+1, \"from\": ri, \"to\": rj, \"P\": P_ts[t][i,j]})\n    df_tm = pd.DataFrame(tm_rows); f_tm = OUTDIR / f\"transition_matrices_{scenario}_{method}.csv\"; df_tm.to_csv(f_tm, index=False)\n\n    # Save macro paths\n    df_macro = pd.DataFrame({\"t\": np.arange(1, T+1), \"M_forecast\": M_hat, \"M_realised\": M_real, \"M_estimate\": M_est})\n    f_macro = OUTDIR / f\"macro_paths_{scenario}_{method}.csv\"; df_macro.to_csv(f_macro, index=False)\n\n    # Save PD term structures\n    df_pd = pd.DataFrame({\"t\": np.arange(0, T+1), \"Y_t\": Y_t, \"Y_ttc\": Y_ttc})\n    f_pd = OUTDIR / f\"pd_term_structures_{scenario}_{method}.csv\"; df_pd.to_csv(f_pd, index=False)\n\n    # Figure: macro filter (save + show)\n    fig = plt.figure(figsize=(7,3.5))\n    tt = np.arange(1, T+1)\n    plt.plot(tt, M_hat, label=\"Forecast (M̂)\"); plt.plot(tt, M_real, label=\"Realised M\"); plt.plot(tt, M_est, label=f\"Estimate: {method}\")\n    plt.axhline(0.0, color=\"k\", lw=0.7, alpha=0.4); plt.xlabel(\"Quarter\"); plt.ylabel(\"Macro index (z)\"); plt.title(f\"Macro filter: {scenario} × {method}\"); plt.legend()\n    f_fig = OUTDIR / f\"macro_filter_{scenario}_{method}.png\"; fig.savefig(f_fig, bbox_inches=\"tight\"); plt.show()\n\n    return {\"labels0\": labels0, \"P_ts\": P_ts, \"pi_ts\": pi_ts, \"Y_t\": Y_t, \"Y_ttc\": Y_ttc,\n            \"M_hat\": M_hat, \"M_real\": M_real, \"M_est\": M_est,\n            \"files\": {\"transition_matrices\": str(f_tm), \"macro_paths\": str(f_macro), \"pd_term_structures\": str(f_pd), \"macro_figure\": str(f_fig)}}"
  },
  {
    "objectID": "posts/anchored-kf.html#pd-bands-metrics",
    "href": "posts/anchored-kf.html#pd-bands-metrics",
    "title": "Anchored Kalman Filtering in a Markov PD Model — Reproducible Demo",
    "section": "9) PD Bands & Metrics",
    "text": "9) PD Bands & Metrics\nHelper to draw PD bands for a fixed method and compute cross-scenario variance.\n\ndef variance_across_scenarios(Y_by_scn: Dict[str, np.ndarray]) -&gt; pd.DataFrame:\n    Ys = np.stack([v for v in Y_by_scn.values()], axis=0)\n    return pd.DataFrame({\"t\": np.arange(Ys.shape[1]), \"var_Y_t\": Ys.var(axis=0, ddof=0)})\n\nShow PD bands inline + save\n\n# Run three scenarios quickly for \"anchored\"\nscenarios = [\"baseline\", \"stress\", \"pandemic\"]; method = \"anchored\"; T_demo = 20\nresults = {sc: run_experiment(sc, method=method, T=T_demo, seed=2025) for sc in scenarios}\n\ntgrid = np.arange(0, T_demo+1)\nfig, ax = plt.subplots(figsize=(8, 4))\nany_key = next(iter(results)); Y_ttc = results[any_key][\"Y_ttc\"]; ax.plot(tgrid, Y_ttc, label=\"TTC baseline\")\nfor scn, res in results.items(): ax.plot(tgrid, res[\"Y_t\"], label=scn)\nax.set_xlabel(\"Quarter\"); ax.set_ylabel(\"Cumulative PD (π_t[D])\"); ax.set_title(f\"PD term-structure bands — {method}\"); ax.legend()\nf_bands = OUTDIR / f\"pd_bands_{method}.png\"; fig.savefig(f_bands, bbox_inches=\"tight\"); plt.show(); f_bands\n\nVariance across scenarios (display)\n\nvar_df = variance_across_scenarios({sc: res[\"Y_t\"] for sc, res in results.items()})\nvar_df.head(10)"
  },
  {
    "objectID": "posts/anchored-kf.html#main-full-grid-scenarios-methods",
    "href": "posts/anchored-kf.html#main-full-grid-scenarios-methods",
    "title": "Anchored Kalman Filtering in a Markov PD Model — Reproducible Demo",
    "section": "10) Main — Full Grid (Scenarios × Methods)",
    "text": "10) Main — Full Grid (Scenarios × Methods)\nWe run all scenarios × methods, save summary tables, and print sample heads.\n\ndef main():\n    seed = 20250821\n    rng = set_seed(seed)\n    scenarios = [\"baseline\", \"stress\", \"pandemic\"]; methods = [\"raw\", \"naive\", \"anchored\"]\n    pi0 = np.array([0.45, 0.40, 0.15, 0.00], dtype=float)\n    P_TTC = build_P_TTC(); betas = _build_betas()\n\n    all_results: Dict[str, Dict[str, object]] = {m:{} for m in methods}\n    file_registry = []\n\n    # Run experiments\n    for m in methods:\n        for scn in scenarios:\n            res = run_experiment(scn, m, N=N_DEFAULT, T=T_DEFAULT, seed=seed)\n            all_results[m][scn] = res\n            files = res[\"files\"]\n            file_registry.append({\"scenario\": scn, \"method\": m, **files})\n\n    # PD bands per method + variance table\n    var_tables = []; pd_band_files = []\n    for m in methods:\n        # Bands (inline + save)\n        tgrid = np.arange(0, T_DEFAULT+1)\n        fig, ax = plt.subplots(figsize=(8, 4))\n        any_key = next(iter(all_results[m])); Y_ttc = all_results[m][any_key][\"Y_ttc\"]; ax.plot(tgrid, Y_ttc, label=\"TTC baseline\")\n        for scn, res in all_results[m].items(): ax.plot(tgrid, res[\"Y_t\"], label=scn)\n        ax.set_xlabel(\"Quarter\"); ax.set_ylabel(\"Cumulative PD (π_t[D])\"); ax.set_title(f\"PD bands — {m}\"); ax.legend()\n        f = OUTDIR / f\"pd_bands_{m}.png\"; fig.savefig(f, bbox_inches=\"tight\"); plt.show()\n        pd_band_files.append({\"method\": m, \"pd_bands_figure\": str(f)})\n\n        # Variance\n        Y_by_scn = {scn: all_results[m][scn][\"Y_t\"] for scn in scenarios}\n        df_var = variance_across_scenarios(Y_by_scn); df_var.insert(0, \"method\", m); var_tables.append(df_var)\n\n    df_var_all = pd.concat(var_tables, ignore_index=True); f_var = OUTDIR / \"variance_Y_across_scenarios.csv\"; df_var_all.to_csv(f_var, index=False)\n\n    # Monte Carlo lifetime loss volatility per scenario × method (summary only)\n    mc_rows = []\n    for m in methods:\n        for scn in scenarios:\n            base_forecasts = gen_macro_forecasts_and_realised(scn, T=T_DEFAULT, rng=rng)\n            # small MC for speed in-page; you can raise n_rep\n            def mc_once(n_rep=200, seed_off=0):\n                rng_local = np.random.default_rng(seed + zlib.crc32(f\"{scn}|{m}|mc\".encode()) % 10_000 + seed_off)\n                T = len(base_forecasts[\"M_hat\"]); YT = np.zeros(n_rep)\n                for r in range(n_rep):\n                    gdp_f = base_forecasts[\"gdp_forecast\"]; un_f = base_forecasts[\"unemp_forecast\"]\n                    gdp_r = gdp_f + rng_local.normal(0.0, 0.2, size=T); un_r = un_f + rng_local.normal(0.0, 0.2, size=T)\n                    if scn.lower().startswith(\"pandemic\") and T &gt;= 4: gdp_r[3] += 2.5; un_r[3] -= 0.8\n                    M_hat = macro_index_from_gdp_unemp(gdp_f, un_f); M_real = macro_index_from_gdp_unemp(gdp_r, un_r)\n                    if m == \"raw\": M_eff = M_real\n                    elif m == \"naive\": M_eff = kalman_naive(M_hat, rho=0.90, Q=None, R=0.25)\n                    elif m == \"anchored\": M_eff = kalman_anchored(M_hat, T_anchor=T, rho=0.90, Q=None, R=0.25, sigma_star2_pre=0.25)\n                    else: raise ValueError(\"Unknown method\")\n                    P_ts = [pit_overlay(P_TTC, float(M_eff[t]), _build_betas()) for t in range(T)]\n                    pi_ts = propagate_distribution(pi0, P_ts); YT[r] = pi_ts[-1, IDX[\"D\"]]\n                return {\"mean\": float(YT.mean()), \"std\": float(YT.std(ddof=0))}\n            mc = mc_once(n_rep=200)\n            mc_rows.append({\"scenario\": scn, \"method\": m, \"YT_mean\": mc[\"mean\"], \"YT_std\": mc[\"std\"]})\n    df_mc = pd.DataFrame(mc_rows); f_mc = OUTDIR / \"lifetime_loss_volatility_mc.csv\"; df_mc.to_csv(f_mc, index=False)\n\n    # Summary file registry\n    for item in pd_band_files:\n        file_registry.append({\"scenario\":\"all\",\"method\":item[\"method\"],\"macro_paths\":\"\",\"transition_matrices\":\"\",\"pd_term_structures\":\"\",\"macro_figure\":\"\", \"pd_bands_figure\":item[\"pd_bands_figure\"]})\n    df_registry = pd.DataFrame(file_registry); f_registry = OUTDIR / \"file_registry.csv\"; df_registry.to_csv(f_registry, index=False)\n\n    return {\"registry\": str(f_registry), \"variance_table\": str(f_var), \"mc_table\": str(f_mc),\n            \"example_macro\": all_results[\"naive\"][\"baseline\"][\"files\"][\"macro_paths\"],\n            \"example_pd\": all_results[\"anchored\"][\"pandemic\"][\"files\"][\"pd_term_structures\"]}\n\npaths = main()\n\nprint(\"Saved files (selected):\")\nfor k, v in paths.items(): print(f\" - {k}: {v}\")\n\nprint(\"\\nExample heads:\")\ndf_macro = pd.read_csv(paths[\"example_macro\"]); df_pd = pd.read_csv(paths[\"example_pd\"])\nprint(\"\\nmacro_paths (baseline × naive) head:\"); print(df_macro.head(6))\nprint(\"\\npd_term_structures (pandemic × anchored) head:\"); print(df_pd.head(6))"
  },
  {
    "objectID": "posts/anchored-kf.html#appendix-figures-mc-distributions-summaries",
    "href": "posts/anchored-kf.html#appendix-figures-mc-distributions-summaries",
    "title": "Anchored Kalman Filtering in a Markov PD Model — Reproducible Demo",
    "section": "11) Appendix Figures — MC Distributions & Summaries",
    "text": "11) Appendix Figures — MC Distributions & Summaries\nWe ensure per-scenario MC sample CSVs exist, then create four multi-panel figures. Every figure is shown and saved.\n\ndef _stable_offset_key(*parts: str, mod: int = 10_000) -&gt; int:\n    key = \"|\".join(parts).encode(); return zlib.crc32(key) % mod\n\ndef _ensure_mc_samples_csvs(\n    outdir: Path,\n    scenarios = (\"baseline\", \"stress\", \"pandemic\"),\n    methods = (\"raw\", \"naive\", \"anchored\"),\n    n_rep: int = 200,\n    seed_base: int = 2025,\n    T: int = T_DEFAULT,\n    pi0: np.ndarray = np.array([0.45, 0.40, 0.15, 0.00], dtype=float),\n):\n    outdir.mkdir(parents=True, exist_ok=True)\n    P_TTC_local = build_P_TTC(); betas = _build_betas()\n    for scn in scenarios:\n        f_samples = outdir / f\"mc_samples_YT_{scn}.csv\"\n        if f_samples.exists(): continue\n        rng_base = set_seed(seed_base + _stable_offset_key(\"base\", scn))\n        base_forecasts = gen_macro_forecasts_and_realised(scn, T=T, rng=rng_base)\n        data: Dict[str, np.ndarray] = {}\n        for m in methods:\n            mc = []; rng_mc = np.random.default_rng(seed_base + _stable_offset_key(\"mc\", scn, m))\n            for _ in range(n_rep):\n                gdp_f = base_forecasts[\"gdp_forecast\"]; un_f = base_forecasts[\"unemp_forecast\"]\n                gdp_r = gdp_f + rng_mc.normal(0.0, 0.2, size=T); un_r = un_f + rng_mc.normal(0.0, 0.2, size=T)\n                if scn.lower().startswith(\"pandemic\") and T &gt;= 4: gdp_r[3] += 2.5; un_r[3] -= 0.8\n                M_hat = macro_index_from_gdp_unemp(gdp_f, un_f); M_real = macro_index_from_gdp_unemp(gdp_r, un_r)\n                if m == \"raw\": M_eff = M_real\n                elif m == \"naive\": M_eff = kalman_naive(M_hat, rho=0.90, Q=None, R=0.25)\n                elif m == \"anchored\": M_eff = kalman_anchored(M_hat, T_anchor=T, rho=0.90, Q=None, R=0.25, sigma_star2_pre=0.25)\n                else: raise ValueError(\"Unknown method\")\n                P_ts = [pit_overlay(P_TTC_local, float(M_eff[t]), betas) for t in range(T)]\n                pi_ts = propagate_distribution(pi0, P_ts)\n                mc.append(pi_ts[-1, IDX[\"D\"]])\n            data[m] = np.array(mc, dtype=float)\n        pd.DataFrame(data).to_csv(f_samples, index=False)\n\n# Ensure samples exist\n_ensure_mc_samples_csvs(OUTDIR)\n\n11.1 Notched boxplots\n\nscenarios = [\"baseline\", \"stress\", \"pandemic\"]; methods = [\"raw\", \"naive\", \"anchored\"]\nfig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\nfor ax, scn in zip(axes, scenarios):\n    df = pd.read_csv(OUTDIR / f\"mc_samples_YT_{scn}.csv\")\n    ax.boxplot([df[m] for m in methods], labels=methods, notch=True, showfliers=False)\n    ax.set_title(scn.capitalize()); ax.set_xlabel(\"Method\")\naxes[0].set_ylabel(\"Lifetime PD at T\")\nfig.suptitle(\"Notched boxplots — Lifetime PD distributions (200 MC reps)\", y=1.02)\nf_box = OUTDIR / \"combined_notched_boxplots_all.png\"; fig.savefig(f_box, bbox_inches=\"tight\"); plt.show(); f_box\n\n11.2 Violin plots\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\nfor ax, scn in zip(axes, scenarios):\n    df = pd.read_csv(OUTDIR / f\"mc_samples_YT_{scn}.csv\")\n    ax.violinplot([df[m] for m in methods], showmeans=True, showmedians=True)\n    ax.set_xticks(range(1, len(methods)+1)); ax.set_xticklabels(methods)\n    ax.set_title(scn.capitalize()); ax.set_xlabel(\"Method\")\naxes[0].set_ylabel(\"Lifetime PD at T\")\nfig.suptitle(\"Violin plots — Lifetime PD distributions (200 MC reps)\", y=1.02)\nf_viol = OUTDIR / \"combined_violins_all.png\"; fig.savefig(f_viol, bbox_inches=\"tight\"); plt.show(); f_viol\n\n11.3 Jittered box+dot (deterministic jitter)\n\nrng_jitter = np.random.default_rng(12345)\nfig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\nfor ax, scn in zip(axes, scenarios):\n    df = pd.read_csv(OUTDIR / f\"mc_samples_YT_{scn}.csv\")\n    ax.boxplot([df[m] for m in methods], labels=methods, showfliers=False)\n    for i, m in enumerate(methods, start=1):\n        y = df[m].values; x = rng_jitter.normal(i, 0.06, size=len(y))\n        ax.plot(x, y, linestyle=\"\", marker=\"o\", alpha=0.35, markersize=3)\n    ax.set_title(scn.capitalize()); ax.set_xlabel(\"Method\")\naxes[0].set_ylabel(\"Lifetime PD at T\")\nfig.suptitle(\"Jittered box+dot — Lifetime PD distributions (200 MC reps)\", y=1.02)\nf_boxdot = OUTDIR / \"combined_boxdot_all.png\"; fig.savefig(f_boxdot, bbox_inches=\"tight\"); plt.show(); f_boxdot\n\n11.4 Mean ± std bars from summary table\n\ndf_mc = pd.read_csv(OUTDIR / \"lifetime_loss_volatility_mc.csv\")\nfig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\nfor ax, scn in zip(axes, scenarios):\n    sub = df_mc[df_mc[\"scenario\"] == scn].copy()\n    order = [\"raw\", \"naive\", \"anchored\"]\n    sub[\"method\"] = pd.Categorical(sub[\"method\"], categories=order, ordered=True)\n    sub = sub.sort_values(\"method\")\n    x = np.arange(len(order))\n    ax.bar(x, sub[\"YT_mean\"].values, yerr=sub[\"YT_std\"].values, capsize=4)\n    ax.set_xticks(x); ax.set_xticklabels(order)\n    ax.set_title(scn.capitalize()); ax.set_xlabel(\"Method\")\naxes[0].set_ylabel(\"Lifetime PD at T (mean ± std)\")\nfig.suptitle(\"Lifetime PD (200 MC reps) — mean with volatility\", y=1.02)\nf_bars = OUTDIR / \"combined_mc_lifetime_pd_bars_all.png\"; fig.savefig(f_bars, bbox_inches=\"tight\"); plt.show(); f_bars\n\n11.5 Summary of saved figures\n\nprint(\"Saved appendix figures:\")\nprint(\" - Notched boxplots :\", f_box)\nprint(\" - Violin plots     :\", f_viol)\nprint(\" - Jittered box+dot :\", f_boxdot)\nprint(\" - Mean±Std bars    :\", f_bars)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Scenario Convexification (WIP)\nBrief description and goals.\n\n\nLifetime PD & Kalman (WIP)\nBrief description and goals."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Risk modelling and simulation\nOptimisation and control\nApplied AI for decision systems"
  },
  {
    "objectID": "research.html#areas-interests",
    "href": "research.html#areas-interests",
    "title": "Research",
    "section": "",
    "text": "Risk modelling and simulation\nOptimisation and control\nApplied AI for decision systems"
  },
  {
    "objectID": "research.html#software-code-links-later",
    "href": "research.html#software-code-links-later",
    "title": "Research",
    "section": "Software / Code (links later)",
    "text": "Software / Code (links later)\n\nScenario Convexification — simulation package (WIP)\nLifetime PD & Kalman filtering — demo notebooks (WIP)"
  },
  {
    "objectID": "research.html#talks",
    "href": "research.html#talks",
    "title": "Research",
    "section": "Talks",
    "text": "Talks\n\nSlides and notes (coming soon)"
  }
]